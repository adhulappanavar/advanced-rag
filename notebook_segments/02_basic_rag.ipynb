{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔍 Step 4: Basic RAG Implementation\n",
        "\n",
        "Now we'll set up the basic RAG pipeline with OpenAI LLM and Pinecone retriever.\n",
        "\n",
        "**Progress**: Initializing LLM and creating basic RAG function..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ OpenAI API key loaded successfully\n",
            "✅ OpenAI LLM initialized successfully\n",
            "🤖 Model: gpt-4o-mini\n",
            "🌡️ Temperature: 0.1\n",
            "📡 Streaming: False\n",
            "\n",
            "🧪 Test Response: LLM is working correctly.\n",
            "✅ LLM is ready to use!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/anidhula/.pyenv/versions/3.12.2/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Setup complete! Pinecone retriever ready.\n",
            "📊 Index name: advance-rag\n",
            "🔍 Search type: similarity, k=5\n",
            "🤖 Embedding model: intfloat/multilingual-e5-large\n"
          ]
        }
      ],
      "source": [
        "# Basic RAG Implementation\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
        "import numpy as np\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from pinecone import Pinecone\n",
        "\n",
        "load_dotenv()\n",
        "# Get OpenAI API key\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "if not openai_api_key:\n",
        "    raise ValueError(\"OPENAI_API_KEY not found in environment variables or .env file\")\n",
        "\n",
        "print(\"✅ OpenAI API key loaded successfully\")\n",
        "\n",
        "# Initialize OpenAI LLM\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",  # Using GPT-4o-mini for cost efficiency\n",
        "    openai_api_key=openai_api_key,\n",
        "    temperature=0.1,\n",
        "    streaming=False\n",
        ")\n",
        "\n",
        "print(\"✅ OpenAI LLM initialized successfully\")\n",
        "print(f\"🤖 Model: gpt-4o-mini\")\n",
        "print(f\"🌡️ Temperature: 0.1\")\n",
        "print(f\"📡 Streaming: False\")\n",
        "\n",
        "# Test the LLM with a simple query\n",
        "try:\n",
        "    test_response = llm.invoke(\"Hello! Please respond with 'LLM is working correctly.'\")\n",
        "    print(f\"\\n🧪 Test Response: {test_response.content}\")\n",
        "    print(\"✅ LLM is ready to use!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error testing LLM: {str(e)}\")\n",
        "    print(\"Please check your API key and internet connection.\")\n",
        "\n",
        "# Initialize Pinecone\n",
        "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
        "index_name = os.getenv(\"PINECONE_INDEX\")\n",
        "index = pc.Index(index_name)\n",
        "\n",
        "# Initialize embedding model\n",
        "embedding_model = SentenceTransformerEmbeddings(\n",
        "    model_name='intfloat/multilingual-e5-large'\n",
        ")\n",
        "\n",
        "# Create VectorStore\n",
        "vectorstore = PineconeVectorStore(\n",
        "    index=index,\n",
        "    embedding=embedding_model\n",
        ")\n",
        "\n",
        "# Create retriever with similarity search and k=5\n",
        "retriever = vectorstore.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 5}\n",
        ")\n",
        "\n",
        "print(\"✅ Setup complete! Pinecone retriever ready.\")\n",
        "print(f\"📊 Index name: {index_name}\")\n",
        "print(f\"🔍 Search type: similarity, k=5\")\n",
        "print(f\"🤖 Embedding model: intfloat/multilingual-e5-large\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧪 Testing basic RAG function...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/anidhula/.pyenv/versions/3.12.2/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Retrieved 5 chunks using direct Pinecone query\n",
            "\n",
            "📋 RETRIEVED CHUNKS METADATA:\n",
            "--------------------------------------------------\n",
            "Chunk 1: Apple (2024.0) - ID: apple_2024_financial_statements_apple_2024_business_13 - Business\n",
            "Chunk 2: Apple (2024.0) - ID: apple_2024_financial_statements_apple_2024_business_13 - Business\n",
            "Chunk 3: Apple (2024.0) - ID: apple_2024_financial_statements_apple_2024_business_13 - Business\n",
            "Chunk 4: Apple (2024.0) - ID: apple_2024_financial_statements_apple_2024_general_307 - General\n",
            "Chunk 5: Apple (2024.0) - ID: apple_2024_financial_statements_apple_2024_general_307 - General\n",
            "\n",
            "🤖 OpenAI LLM Response Generated (2504 characters)\n",
            "\n",
            "================================================================================\n",
            "🎯 BASIC RAG ANSWER:\n",
            "================================================================================\n",
            "The Apple Inc. Form 10-K for the fiscal year ended September 29, 2018, includes several key sections that provide a comprehensive overview of the company's business operations, financial performance, and risk factors. Here are the summarized key points:\n",
            "\n",
            "### Part I\n",
            "1. **Business (Item 1)**: \n",
            "   - Apple operates in the technology sector, primarily focusing on designing, manufacturing, and marketing consumer electronics, software, and services. Key products include the iPhone, iPad, Mac, Apple Watch, and Apple TV, along with services like the App Store, Apple Music, and iCloud.\n",
            "\n",
            "2. **Risk Factors (Item 1A)**: \n",
            "   - The company outlines various risks that could impact its business, including competition, reliance on key suppliers, potential legal issues, and the impact of global economic conditions.\n",
            "\n",
            "3. **Unresolved Staff Comments (Item 1B)**: \n",
            "   - This section typically addresses any comments from the SEC that remain unresolved, although specific details are not provided in the context.\n",
            "\n",
            "4. **Properties (Item 2)**: \n",
            "   - Apple owns and leases various properties worldwide, including corporate offices, retail stores, and manufacturing facilities.\n",
            "\n",
            "5. **Legal Proceedings (Item 3)**: \n",
            "   - The company is involved in various legal proceedings, which are common for large corporations, but specific cases are not detailed in the context.\n",
            "\n",
            "6. **Mine Safety Disclosures (Item 4)**: \n",
            "   - This section is generally relevant for companies involved in mining, and it may not apply significantly to Apple.\n",
            "\n",
            "### Part II\n",
            "1. **Market for Registrant’s Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities (Item 5)**: \n",
            "   - This section discusses the market performance of Apple’s stock, including stockholder matters and any repurchase of equity securities.\n",
            "\n",
            "2. **Selected Financial Data (Item 6)**: \n",
            "   - This section provides a summary of key financial metrics, including revenue, net income, and earnings per share, although specific figures are not included in the context.\n",
            "\n",
            "3. **Management’s Discussion and Analysis (Item 7)**: \n",
            "   - This section typically includes management's insights on financial results, operational performance, and future outlook, but specific details are not provided in the context.\n",
            "\n",
            "### Conclusion\n",
            "The 10-K report serves as a comprehensive document that outlines Apple's business model, financial health, and the risks it faces. For detailed financial figures and specific legal proceedings, one would need to refer to the full document.\n",
            "================================================================================\n",
            "\n",
            "✅ Step 4 Complete: Basic RAG implementation finished!\n"
          ]
        }
      ],
      "source": [
        "def get_rag_answer(query: str, retriever, llm) -> str:\n",
        "    \"\"\"\n",
        "    Retrieve relevant chunks and generate answer using OpenAI LLM\n",
        "\n",
        "    Args:\n",
        "        query: User's question\n",
        "        retriever: Pinecone retriever or custom retrieval function\n",
        "        llm: OpenAI LLM instance\n",
        "\n",
        "    Returns:\n",
        "        Generated answer based on retrieved context\n",
        "    \"\"\"\n",
        "\n",
        "    # Method 1: Try using LangChain retriever first\n",
        "    try:\n",
        "        query_embedding = embedding_model.embed_query(query)\n",
        "        response = index.query(\n",
        "            vector=query_embedding,\n",
        "            top_k=5,\n",
        "            include_metadata=True,\n",
        "            include_values=False\n",
        "        )\n",
        "        chunks = []\n",
        "        metadata_list = []\n",
        "        for match in response.matches:\n",
        "            chunk_text = match.metadata.get('chunk_text', '')\n",
        "            if chunk_text:\n",
        "                chunks.append(chunk_text)\n",
        "                metadata_list.append(match.metadata)\n",
        "        print(f\"✅ Retrieved {len(chunks)} chunks using direct Pinecone query\")\n",
        "        if not chunks:\n",
        "          return \"No relevant information found in the database.\"\n",
        "    except Exception as e:\n",
        "         print(\"Exception\",e)\n",
        "    # Print retrieved metadata for transparency\n",
        "    print(\"\\n📋 RETRIEVED CHUNKS METADATA:\")\n",
        "    print(\"-\" * 50)\n",
        "    for i, metadata in enumerate(metadata_list, 1):\n",
        "        company = metadata.get('company', 'Unknown').replace(' (1)', '')\n",
        "        year = metadata.get('year', 'Unknown')\n",
        "        chunk_id = metadata.get('chunk_id', 'Unknown')\n",
        "        source = metadata.get('section', 'Unknown')\n",
        "        chunk_text=metadata.get('chunk_text', 'Unknown')\n",
        "        print(f\"Chunk {i}: {company.title()} ({year}) - ID: {chunk_id} - {source}\")\n",
        "        #print(f\"Chunk text {i}: {chunk_text}\")\n",
        "\n",
        "    # Combine chunks into context\n",
        "    context = \"\\n\\n\".join([f\"Document {i+1}:\\n{chunk}\" for i, chunk in enumerate(chunks)])\n",
        "\n",
        "    # Create prompt for OpenAI\n",
        "    prompt = f\"\"\"Based on the following documents, please answer the user's question accurately and comprehensively.\n",
        "\n",
        "QUESTION: {query}\n",
        "\n",
        "CONTEXT DOCUMENTS:\n",
        "{context}\n",
        "\n",
        "INSTRUCTIONS:\n",
        "- Use only the information provided in the context documents\n",
        "- If the information is not sufficient to answer the question, state this clearly\n",
        "- Provide specific details and numbers when available\n",
        "- Structure your answer clearly and concisely\n",
        "- If data spans multiple years or sources, organize it logically\n",
        "\n",
        "ANSWER:\"\"\"\n",
        "\n",
        "    # Send to OpenAI LLM\n",
        "    try:\n",
        "        response = llm.invoke(prompt)\n",
        "        answer = response.content.strip()\n",
        "\n",
        "        print(f\"\\n🤖 OpenAI LLM Response Generated ({len(answer)} characters)\")\n",
        "        return answer\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error generating answer with OpenAI: {str(e)}\"\n",
        "\n",
        "# Test the basic RAG function\n",
        "print(\"🧪 Testing basic RAG function...\")\n",
        "test_query = \"Summarize key points from Apple 10k.\"\n",
        "final_answer = get_rag_answer(test_query, retriever, llm)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"🎯 BASIC RAG ANSWER:\")\n",
        "print(\"=\"*80)\n",
        "print(final_answer)\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n✅ Step 4 Complete: Basic RAG implementation finished!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
