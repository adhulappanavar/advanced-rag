{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ” Step 4: Basic RAG Implementation\n",
        "\n",
        "Now we'll set up the basic RAG pipeline with OpenAI LLM and Pinecone retriever.\n",
        "\n",
        "**Progress**: Initializing LLM and creating basic RAG function..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… OpenAI API key loaded successfully\n",
            "âœ… OpenAI LLM initialized successfully\n",
            "ðŸ¤– Model: gpt-4o-mini\n",
            "ðŸŒ¡ï¸ Temperature: 0.1\n",
            "ðŸ“¡ Streaming: False\n",
            "\n",
            "ðŸ§ª Test Response: LLM is working correctly.\n",
            "âœ… LLM is ready to use!\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'Pinecone' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     39\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPlease check your API key and internet connection.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Initialize Pinecone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m pc = \u001b[43mPinecone\u001b[49m(api_key=os.getenv(\u001b[33m\"\u001b[39m\u001b[33mPINECONE_API_KEY\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     43\u001b[39m index_name = os.getenv(\u001b[33m\"\u001b[39m\u001b[33mPINECONE_INDEX\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     44\u001b[39m index = pc.Index(index_name)\n",
            "\u001b[31mNameError\u001b[39m: name 'Pinecone' is not defined"
          ]
        }
      ],
      "source": [
        "# Basic RAG Implementation\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
        "import numpy as np\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from pinecone import Pinecone\n",
        "\n",
        "load_dotenv()\n",
        "# Get OpenAI API key\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "if not openai_api_key:\n",
        "    raise ValueError(\"OPENAI_API_KEY not found in environment variables or .env file\")\n",
        "\n",
        "print(\"âœ… OpenAI API key loaded successfully\")\n",
        "\n",
        "# Initialize OpenAI LLM\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",  # Using GPT-4o-mini for cost efficiency\n",
        "    openai_api_key=openai_api_key,\n",
        "    temperature=0.1,\n",
        "    streaming=False\n",
        ")\n",
        "\n",
        "print(\"âœ… OpenAI LLM initialized successfully\")\n",
        "print(f\"ðŸ¤– Model: gpt-4o-mini\")\n",
        "print(f\"ðŸŒ¡ï¸ Temperature: 0.1\")\n",
        "print(f\"ðŸ“¡ Streaming: False\")\n",
        "\n",
        "# Test the LLM with a simple query\n",
        "try:\n",
        "    test_response = llm.invoke(\"Hello! Please respond with 'LLM is working correctly.'\")\n",
        "    print(f\"\\nðŸ§ª Test Response: {test_response.content}\")\n",
        "    print(\"âœ… LLM is ready to use!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error testing LLM: {str(e)}\")\n",
        "    print(\"Please check your API key and internet connection.\")\n",
        "\n",
        "# Initialize Pinecone\n",
        "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
        "index_name = os.getenv(\"PINECONE_INDEX\")\n",
        "index = pc.Index(index_name)\n",
        "\n",
        "# Initialize embedding model\n",
        "embedding_model = SentenceTransformerEmbeddings(\n",
        "    model_name='intfloat/multilingual-e5-large'\n",
        ")\n",
        "\n",
        "# Create VectorStore\n",
        "vectorstore = PineconeVectorStore(\n",
        "    index=index,\n",
        "    embedding=embedding_model\n",
        ")\n",
        "\n",
        "# Create retriever with similarity search and k=5\n",
        "retriever = vectorstore.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 5}\n",
        ")\n",
        "\n",
        "print(\"âœ… Setup complete! Pinecone retriever ready.\")\n",
        "print(f\"ðŸ“Š Index name: {index_name}\")\n",
        "print(f\"ðŸ” Search type: similarity, k=5\")\n",
        "print(f\"ðŸ¤– Embedding model: intfloat/multilingual-e5-large\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ§ª Testing basic RAG function...\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'retriever' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 81\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mðŸ§ª Testing basic RAG function...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     80\u001b[39m test_query = \u001b[33m\"\u001b[39m\u001b[33mSummarize key points from Apple 10k.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m final_answer = get_rag_answer(test_query, \u001b[43mretriever\u001b[49m, llm)\n\u001b[32m     82\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n\u001b[32m     83\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mðŸŽ¯ BASIC RAG ANSWER:\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'retriever' is not defined"
          ]
        }
      ],
      "source": [
        "def get_rag_answer(query: str, retriever, llm) -> str:\n",
        "    \"\"\"\n",
        "    Retrieve relevant chunks and generate answer using OpenAI LLM\n",
        "\n",
        "    Args:\n",
        "        query: User's question\n",
        "        retriever: Pinecone retriever or custom retrieval function\n",
        "        llm: OpenAI LLM instance\n",
        "\n",
        "    Returns:\n",
        "        Generated answer based on retrieved context\n",
        "    \"\"\"\n",
        "\n",
        "    # Method 1: Try using LangChain retriever first\n",
        "    try:\n",
        "        query_embedding = embedding_model.embed_query(query)\n",
        "        response = index.query(\n",
        "            vector=query_embedding,\n",
        "            top_k=5,\n",
        "            include_metadata=True,\n",
        "            include_values=False\n",
        "        )\n",
        "        chunks = []\n",
        "        metadata_list = []\n",
        "        for match in response.matches:\n",
        "            chunk_text = match.metadata.get('chunk_text', '')\n",
        "            if chunk_text:\n",
        "                chunks.append(chunk_text)\n",
        "                metadata_list.append(match.metadata)\n",
        "        print(f\"âœ… Retrieved {len(chunks)} chunks using direct Pinecone query\")\n",
        "        if not chunks:\n",
        "          return \"No relevant information found in the database.\"\n",
        "    except Exception as e:\n",
        "         print(\"Exception\",e)\n",
        "    # Print retrieved metadata for transparency\n",
        "    print(\"\\nðŸ“‹ RETRIEVED CHUNKS METADATA:\")\n",
        "    print(\"-\" * 50)\n",
        "    for i, metadata in enumerate(metadata_list, 1):\n",
        "        company = metadata.get('company', 'Unknown').replace(' (1)', '')\n",
        "        year = metadata.get('year', 'Unknown')\n",
        "        chunk_id = metadata.get('chunk_id', 'Unknown')\n",
        "        source = metadata.get('section', 'Unknown')\n",
        "        chunk_text=metadata.get('chunk_text', 'Unknown')\n",
        "        print(f\"Chunk {i}: {company.title()} ({year}) - ID: {chunk_id} - {source}\")\n",
        "        #print(f\"Chunk text {i}: {chunk_text}\")\n",
        "\n",
        "    # Combine chunks into context\n",
        "    context = \"\\n\\n\".join([f\"Document {i+1}:\\n{chunk}\" for i, chunk in enumerate(chunks)])\n",
        "\n",
        "    # Create prompt for OpenAI\n",
        "    prompt = f\"\"\"Based on the following documents, please answer the user's question accurately and comprehensively.\n",
        "\n",
        "QUESTION: {query}\n",
        "\n",
        "CONTEXT DOCUMENTS:\n",
        "{context}\n",
        "\n",
        "INSTRUCTIONS:\n",
        "- Use only the information provided in the context documents\n",
        "- If the information is not sufficient to answer the question, state this clearly\n",
        "- Provide specific details and numbers when available\n",
        "- Structure your answer clearly and concisely\n",
        "- If data spans multiple years or sources, organize it logically\n",
        "\n",
        "ANSWER:\"\"\"\n",
        "\n",
        "    # Send to OpenAI LLM\n",
        "    try:\n",
        "        response = llm.invoke(prompt)\n",
        "        answer = response.content.strip()\n",
        "\n",
        "        print(f\"\\nðŸ¤– OpenAI LLM Response Generated ({len(answer)} characters)\")\n",
        "        return answer\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error generating answer with OpenAI: {str(e)}\"\n",
        "\n",
        "# Test the basic RAG function\n",
        "print(\"ðŸ§ª Testing basic RAG function...\")\n",
        "test_query = \"Summarize key points from Apple 10k.\"\n",
        "final_answer = get_rag_answer(test_query, retriever, llm)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ðŸŽ¯ BASIC RAG ANSWER:\")\n",
        "print(\"=\"*80)\n",
        "print(final_answer)\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nâœ… Step 4 Complete: Basic RAG implementation finished!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
