{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÑ Step 5: Re-ranking with Cohere\n",
        "\n",
        "Now we'll implement re-ranking using Cohere's cross-encoder model to improve retrieval quality.\n",
        "\n",
        "**Progress**: Setting up Cohere re-ranker and implementing re-ranking function..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment Variables Status:\n",
            "------------------------------\n",
            "‚úÖ PINECONE_API_KEY: Set\n",
            "‚úÖ PINECONE_INDEX: Set\n",
            "‚úÖ PINECONE_URL: Set\n",
            "‚úÖ OPENAI_API_KEY: Set\n",
            "‚úÖ COHERE_API_KEY: Set\n",
            "\n",
            "üéâ All environment variables loaded successfully!\n",
            "üìã Pinecone Index: advance-rag\n",
            "‚úÖ OpenAI LLM initialized successfully\n",
            "ü§ñ Model: gpt-4o-mini\n",
            "üå°Ô∏è Temperature: 0.1\n",
            "üì° Streaming: False\n",
            "‚úÖ OpenAI API key loaded successfully\n",
            "üß™ Testing re-ranking function...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/anidhula/.pyenv/versions/3.12.2/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Retrieved 5 chunks using direct Pinecone query\n",
            "\n",
            "üìã RETRIEVED CHUNKS METADATA:\n",
            "--------------------------------------------------\n",
            "Chunk 1: Tesla (2024.0) - ID: tesla_2024_financial_statements_tesla_2024_financial_statements_530 - Financial Statements\n",
            "Chunk 2: Tesla (2024.0) - ID: tesla_2024_financial_statements_tesla_2024_financial_statements_530 - Financial Statements\n",
            "Chunk 3: Tesla (2024.0) - ID: tesla_2024_financial_statements_tesla_2024_financial_statements_530 - Financial Statements\n",
            "Chunk 4: Nvidia (2024.0) - ID: nvidia_2024_financial_statements_nvidia_2024_business_635 - Business\n",
            "Chunk 5: Nvidia (2024.0) - ID: nvidia_2024_financial_statements_nvidia_2024_business_635 - Business\n",
            "\n",
            "ü§ñ DeepSeek LLM Response Generated (553 characters)\n",
            "\n",
            "================================================================================\n",
            "üéØ BASIC RAG ANSWER:\n",
            "================================================================================\n",
            "The context documents do not provide specific information regarding Amazon's R&D spending in 2024. However, they do mention that during fiscal year 2024, Amazon spent $1.1 billion on capital expenditures. There is no direct mention of R&D expenses for that year, nor any details about changes or trends in R&D spending specifically for 2024.\n",
            "\n",
            "In summary, while we know that Amazon spent $1.1 billion on capital expenditures in 2024, there is insufficient information in the provided documents to accurately summarize Amazon's R&D spending for that year.\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/anidhula/.pyenv/versions/3.12.2/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Retrieved 10 chunks using direct Pinecone query\n",
            "\n",
            "üìã CHUNKS BEFORE RE-RANKING:\n",
            "--------------------------------------------------\n",
            "Chunk 1: Tesla (2024.0) - ID: tesla_2024_financial_statements_tesla_2024_financial_statements_530 - Financial Statements\n",
            "Chunk 2: Tesla (2024.0) - ID: tesla_2024_financial_statements_tesla_2024_financial_statements_530 - Financial Statements\n",
            "Chunk 3: Tesla (2024.0) - ID: tesla_2024_financial_statements_tesla_2024_financial_statements_530 - Financial Statements\n",
            "Chunk 4: Nvidia (2024.0) - ID: nvidia_2024_financial_statements_nvidia_2024_business_635 - Business\n",
            "Chunk 5: Nvidia (2024.0) - ID: nvidia_2024_financial_statements_nvidia_2024_business_635 - Business\n",
            "Chunk 6: Nvidia (2024.0) - ID: nvidia_2024_financial_statements_nvidia_2024_business_635 - Business\n",
            "Chunk 7: Nvidia (2024.0) - ID: nvidia_2024_financial_statements_nvidia_2024_executive_compensation_609 - Executive Compensation\n",
            "Chunk 8: Nvidia (2024.0) - ID: nvidia_2024_financial_statements_nvidia_2024_executive_compensation_609 - Executive Compensation\n",
            "Chunk 9: Nvidia (2024.0) - ID: nvidia_2024_financial_statements_nvidia_2024_executive_compensation_609 - Executive Compensation\n",
            "Chunk 10: Nvidia (2024.0) - ID: nvidia_2024_financial_statements_nvidia_2024_executive_compensation_610 - Executive Compensation\n",
            "‚úÖ Re-ranked to top 5 most relevant chunks\n",
            "\n",
            "üìã CHUNKS AFTER RE-RANKING:\n",
            "--------------------------------------------------\n",
            "Chunk 1: Nvidia (2024.0) - ID: nvidia_2024_financial_statements_nvidia_2024_executive_compensation_609 - Executive Compensation\n",
            "Chunk 2: Nvidia (2024.0) - ID: nvidia_2024_financial_statements_nvidia_2024_executive_compensation_609 - Executive Compensation\n",
            "Chunk 3: Nvidia (2024.0) - ID: nvidia_2024_financial_statements_nvidia_2024_executive_compensation_609 - Executive Compensation\n",
            "Chunk 4: Nvidia (2024.0) - ID: nvidia_2024_financial_statements_nvidia_2024_business_635 - Business\n",
            "Chunk 5: Nvidia (2024.0) - ID: nvidia_2024_financial_statements_nvidia_2024_business_635 - Business\n",
            "\n",
            "ü§ñ OpenAI LLM Response Generated (591 characters)\n",
            "\n",
            "================================================================================\n",
            "üéØ RE-RANKED RAG ANSWER:\n",
            "================================================================================\n",
            "In fiscal year 2024, Amazon's research and development (R&D) expenses amounted to $8.675 billion. This represents a significant increase of $4.239 billion, or 49%, compared to the previous fiscal year. As a percentage of net revenue, R&D spending constituted 14.2% in 2024. \n",
            "\n",
            "For context, in fiscal year 2025, R&D expenses are projected to rise to $12.914 billion, which would decrease the percentage of net revenue allocated to R&D to 9.9%. \n",
            "\n",
            "Overall, Amazon's R&D spending in 2024 reflects a strong commitment to innovation and development, with a notable increase compared to prior years.\n",
            "================================================================================\n",
            "\n",
            "üîç Answer Evaluation Completed (3122 characters)\n",
            "\n",
            "================================================================================\n",
            "üìä ANSWER COMPARISON EVALUATION\n",
            "================================================================================\n",
            "**WINNER: Answer 2**\n",
            "\n",
            "**DETAILED ANALYSIS:**\n",
            "\n",
            "**Accuracy & Factual Correctness:**\n",
            "- Answer 1: States that there is no specific information regarding Amazon's R&D spending in 2024 and mentions only capital expenditures of $1.1 billion, which is not related to R&D.\n",
            "- Answer 2: Provides a specific figure of $8.675 billion for R&D expenses in 2024, along with a year-over-year increase and percentage of net revenue, which is accurate and relevant.\n",
            "- Winner: Answer 2 - Answer 2 contains accurate and specific information about R&D spending, while Answer 1 lacks relevant data.\n",
            "\n",
            "**Completeness & Comprehensiveness:**\n",
            "- Answer 1: Lacks comprehensive details about R&D spending, only mentioning capital expenditures without addressing R&D specifically.\n",
            "- Answer 2: Offers a complete overview of R&D spending, including comparisons to previous years and projections for the next year.\n",
            "- Winner: Answer 2 - It provides a thorough summary of R&D spending, including trends and future projections.\n",
            "\n",
            "**Relevance & Focus:**\n",
            "- Answer 1: Focuses on the absence of information regarding R&D spending, which is not directly answering the query.\n",
            "- Answer 2: Directly addresses the question about R&D spending in 2024 and provides relevant context.\n",
            "- Winner: Answer 2 - It stays focused on the specific question asked and provides pertinent information.\n",
            "\n",
            "**Clarity & Organization:**\n",
            "- Answer 1: While clear, it is somewhat disorganized as it emphasizes the lack of information rather than providing a summary.\n",
            "- Answer 2: Clearly structured, presenting the R&D spending figure, the increase, and the percentage of revenue in a logical manner.\n",
            "- Winner: Answer 2 - It is clearer and better organized, making it easier to understand the key points.\n",
            "\n",
            "**Specific Details & Evidence:**\n",
            "- Answer 1: Lacks specific details about R&D spending, providing only capital expenditure figures.\n",
            "- Answer 2: Includes specific numbers, percentage increases, and projections, which support its claims effectively.\n",
            "- Winner: Answer 2 - It provides concrete examples and detailed figures that enhance the response's credibility.\n",
            "\n",
            "**KEY DIFFERENCES:**\n",
            "1. Answer 1 lacks specific information about R&D spending, while Answer 2 provides a detailed figure.\n",
            "2. Answer 1 focuses on capital expenditures, which are not relevant to the query, whereas Answer 2 directly addresses R&D.\n",
            "3. Answer 2 includes comparative data and future projections, making it more comprehensive than Answer 1.\n",
            "4. Answer 2 is better organized and clearer in its presentation of information.\n",
            "\n",
            "**FINAL VERDICT:**\n",
            "- Overall Winner: Answer 2\n",
            "- Confidence Level: High\n",
            "- Main Reasons: Answer 2 provides accurate and specific information about R&D spending, is comprehensive in its coverage, and maintains relevance and clarity throughout.\n",
            "\n",
            "**RECOMMENDATIONS:**\n",
            "- For Answer 1: Include specific figures related to R&D spending and avoid focusing on unrelated capital expenditures. Providing context or comparisons would enhance its value.\n",
            "- For both answers: Ensure that the information presented is directly relevant to the query and structured in a way that highlights key points effectively.\n",
            "================================================================================\n",
            "\n",
            "‚úÖ Step 5 Complete: Re-ranking implementation finished!\n"
          ]
        }
      ],
      "source": [
        "# Re-ranking with Cohere\n",
        "import cohere\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
        "import numpy as np\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import re\n",
        "from typing import Dict, List, Tuple\n",
        "import uuid\n",
        "from pinecone import Pinecone\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Verify environment variables are loaded\n",
        "required_vars = ['PINECONE_API_KEY', 'PINECONE_INDEX', 'PINECONE_URL', 'OPENAI_API_KEY','COHERE_API_KEY']\n",
        "\n",
        "print(\"Environment Variables Status:\")\n",
        "print(\"-\" * 30)\n",
        "for var in required_vars:\n",
        "    value = os.getenv(var)\n",
        "    if value:\n",
        "        print(f\"‚úÖ {var}: Set\")\n",
        "    else:\n",
        "        print(f\"‚ùå {var}: Missing\")\n",
        "\n",
        "# Check if all required variables are present\n",
        "missing_vars = [var for var in required_vars if not os.getenv(var)]\n",
        "\n",
        "if missing_vars:\n",
        "    print(f\"\\n‚ùå Missing variables: {missing_vars}\")\n",
        "    print(\"Please create a .env file and add all required variables\")\n",
        "else:\n",
        "    print(f\"\\nüéâ All environment variables loaded successfully!\")\n",
        "    print(f\"üìã Pinecone Index: {os.getenv('PINECONE_INDEX')}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"‚úÖ OpenAI LLM initialized successfully\")\n",
        "print(f\"ü§ñ Model: gpt-4o-mini\")\n",
        "print(f\"üå°Ô∏è Temperature: 0.1\")\n",
        "print(f\"üì° Streaming: False\")\n",
        "\n",
        "# Get OpenAI API key\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "if not openai_api_key:\n",
        "    raise ValueError(\"OPENAI_API_KEY not found in environment variables or .env file\")\n",
        "\n",
        "print(\"‚úÖ OpenAI API key loaded successfully\")\n",
        "\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",  # Using GPT-4o-mini for cost efficiency\n",
        "    openai_api_key=openai_api_key,\n",
        "    temperature=0.1,\n",
        "    streaming=False\n",
        ")\n",
        "# Initialize Pinecone\n",
        "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
        "index_name = os.getenv(\"PINECONE_INDEX\")\n",
        "index = pc.Index(index_name)\n",
        "\n",
        "# Initialize embedding model\n",
        "embedding_model = SentenceTransformerEmbeddings(\n",
        "    model_name='intfloat/multilingual-e5-large'\n",
        ")\n",
        "\n",
        "# Create VectorStore\n",
        "vectorstore = PineconeVectorStore(\n",
        "    index=index,\n",
        "    embedding=embedding_model\n",
        ")\n",
        "\n",
        "# Create retriever with similarity search and k=5\n",
        "retriever = vectorstore.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 5}\n",
        ")\n",
        "\n",
        "\n",
        "def get_rag_answer(query: str, retriever, llm) -> str:\n",
        "    \"\"\"\n",
        "    Retrieve relevant chunks and generate answer using DeepSeek LLM\n",
        "\n",
        "    Args:\n",
        "        query: User's question\n",
        "        retriever: Pinecone retriever or custom retrieval function\n",
        "        llm: DeepSeek LLM instance\n",
        "\n",
        "    Returns:\n",
        "        Generated answer based on retrieved context\n",
        "    \"\"\"\n",
        "\n",
        "    # Method 1: Try using LangChain retriever first\n",
        "    try:\n",
        "        query_embedding = embedding_model.embed_query(query)\n",
        "        response = index.query(\n",
        "            vector=query_embedding,\n",
        "            top_k=5,\n",
        "            include_metadata=True,\n",
        "            include_values=False\n",
        "        )\n",
        "        chunks = []\n",
        "        metadata_list = []\n",
        "        for match in response.matches:\n",
        "            chunk_text = match.metadata.get('chunk_text', '')\n",
        "            if chunk_text:\n",
        "                chunks.append(chunk_text)\n",
        "                metadata_list.append(match.metadata)\n",
        "        print(f\"‚úÖ Retrieved {len(chunks)} chunks using direct Pinecone query\")\n",
        "        if not chunks:\n",
        "          return \"No relevant information found in the database.\"\n",
        "    except Exception as e:\n",
        "         print(\"Exception\",e)\n",
        "    # Print retrieved metadata for transparency\n",
        "    print(\"\\nüìã RETRIEVED CHUNKS METADATA:\")\n",
        "    print(\"-\" * 50)\n",
        "    for i, metadata in enumerate(metadata_list, 1):\n",
        "        company = metadata.get('company', 'Unknown').replace(' (1)', '')\n",
        "        year = metadata.get('year', 'Unknown')\n",
        "        chunk_id = metadata.get('chunk_id', 'Unknown')\n",
        "        source = metadata.get('section', 'Unknown')\n",
        "        chunk_text=metadata.get('chunk_text', 'Unknown')\n",
        "        print(f\"Chunk {i}: {company.title()} ({year}) - ID: {chunk_id} - {source}\")\n",
        "        #print(f\"Chunk text {i}: {chunk_text}\")\n",
        "\n",
        "    # Combine chunks into context\n",
        "    context = \"\\n\\n\".join([f\"Document {i+1}:\\n{chunk}\" for i, chunk in enumerate(chunks)])\n",
        "\n",
        "    # Create prompt for DeepSeek\n",
        "    prompt = f\"\"\"Based on the following documents, please answer the user's question accurately and comprehensively.\n",
        "\n",
        "QUESTION: {query}\n",
        "\n",
        "CONTEXT DOCUMENTS:\n",
        "{context}\n",
        "\n",
        "INSTRUCTIONS:\n",
        "- Use only the information provided in the context documents\n",
        "- If the information is not sufficient to answer the question, state this clearly\n",
        "- Provide specific details and numbers when available\n",
        "- Structure your answer clearly and concisely\n",
        "- If data spans multiple years or sources, organize it logically\n",
        "\n",
        "ANSWER:\"\"\"\n",
        "\n",
        "    # Send to DeepSeek LLM\n",
        "    try:\n",
        "        response = llm.invoke(prompt)\n",
        "        answer = response.content.strip()\n",
        "\n",
        "        print(f\"\\nü§ñ DeepSeek LLM Response Generated ({len(answer)} characters)\")\n",
        "        return answer\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error generating answer with DeepSeek: {str(e)}\"\n",
        "\n",
        "\n",
        "\n",
        "def get_rag_answer_with_cohere_rerank(query: str, retriever, llm) -> str:\n",
        "    \"\"\"\n",
        "    Retrieve relevant chunks, re-rank them using Cohere, and generate answer using OpenAI LLM\n",
        "\n",
        "    Args:\n",
        "        query: User's question\n",
        "        retriever: Pinecone retriever or custom retrieval function\n",
        "        llm: OpenAI LLM instance\n",
        "\n",
        "    Returns:\n",
        "        Generated answer based on re-ranked retrieved context\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize Cohere client\n",
        "    try:\n",
        "        cohere_api_key = os.getenv(\"COHERE_API_KEY\")\n",
        "        if not cohere_api_key:\n",
        "            return \"COHERE_API_KEY not found in environment variables\"\n",
        "\n",
        "        co = cohere.Client(cohere_api_key)\n",
        "    except Exception as e:\n",
        "        return f\"Error initializing Cohere client: {str(e)}\"\n",
        "\n",
        "    # Method 1: Retrieve the chunks\n",
        "    try:\n",
        "        query_embedding = embedding_model.embed_query(query)\n",
        "        response = index.query(\n",
        "            vector=query_embedding,\n",
        "            top_k=10,  # Get more chunks initially for re-ranking\n",
        "            include_metadata=True,\n",
        "            include_values=False\n",
        "        )\n",
        "        chunks = []\n",
        "        metadata_list = []\n",
        "        documents_for_rerank = []\n",
        "\n",
        "        for match in response.matches:\n",
        "            chunk_text = match.metadata.get('chunk_text', '')\n",
        "            if chunk_text:\n",
        "                chunks.append(chunk_text)\n",
        "                metadata_list.append(match.metadata)\n",
        "                documents_for_rerank.append(chunk_text)\n",
        "\n",
        "        print(f\"‚úÖ Retrieved {len(chunks)} chunks using direct Pinecone query\")\n",
        "\n",
        "        if not chunks:\n",
        "            return \"No relevant information found in the database.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Exception\", e)\n",
        "        return f\"Error during retrieval: {str(e)}\"\n",
        "\n",
        "    # Print chunks BEFORE re-ranking\n",
        "    print(\"\\nüìã CHUNKS BEFORE RE-RANKING:\")\n",
        "    print(\"-\" * 50)\n",
        "    for i, metadata in enumerate(metadata_list, 1):\n",
        "        company = metadata.get('company', 'Unknown').replace(' (1)', '')\n",
        "        year = metadata.get('year', 'Unknown')\n",
        "        chunk_id = metadata.get('chunk_id', 'Unknown')\n",
        "        source = metadata.get('section', 'Unknown')\n",
        "        print(f\"Chunk {i}: {company.title()} ({year}) - ID: {chunk_id} - {source}\")\n",
        "\n",
        "    # Re-rank using Cohere\n",
        "    try:\n",
        "        rerank_response = co.rerank(\n",
        "            model='rerank-english-v3.0',\n",
        "            query=query,\n",
        "            documents=documents_for_rerank,\n",
        "            top_n=5,\n",
        "            return_documents=True\n",
        "        )\n",
        "\n",
        "        # Get re-ranked chunks and their metadata\n",
        "        reranked_chunks = []\n",
        "        reranked_metadata = []\n",
        "\n",
        "        for result in rerank_response.results:\n",
        "            original_index = result.index\n",
        "            reranked_chunks.append(chunks[original_index])\n",
        "            reranked_metadata.append(metadata_list[original_index])\n",
        "\n",
        "        print(f\"‚úÖ Re-ranked to top {len(reranked_chunks)} most relevant chunks\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Exception during re-ranking: {e}\")\n",
        "        # Fallback to original chunks if re-ranking fails\n",
        "        reranked_chunks = chunks[:5]\n",
        "        reranked_metadata = metadata_list[:5]\n",
        "\n",
        "    # Print chunks AFTER re-ranking\n",
        "    print(\"\\nüìã CHUNKS AFTER RE-RANKING:\")\n",
        "    print(\"-\" * 50)\n",
        "    for i, metadata in enumerate(reranked_metadata, 1):\n",
        "        company = metadata.get('company', 'Unknown').replace(' (1)', '')\n",
        "        year = metadata.get('year', 'Unknown')\n",
        "        chunk_id = metadata.get('chunk_id', 'Unknown')\n",
        "        source = metadata.get('section', 'Unknown')\n",
        "        chunk_text = metadata.get('chunk_text', 'Unknown')\n",
        "        print(f\"Chunk {i}: {company.title()} ({year}) - ID: {chunk_id} - {source}\")\n",
        "        #print(f\"Chunk text {i}: {chunk_text}\")\n",
        "\n",
        "    # Combine chunks into context\n",
        "    context = \"\\n\\n\".join([f\"Document {i+1}:\\n{chunk}\" for i, chunk in enumerate(reranked_chunks)])\n",
        "\n",
        "    # Create prompt for OpenAI\n",
        "    prompt = f\"\"\"Based on the following documents, please answer the user's question accurately and comprehensively.\n",
        "\n",
        "QUESTION: {query}\n",
        "\n",
        "CONTEXT DOCUMENTS:\n",
        "{context}\n",
        "\n",
        "INSTRUCTIONS:\n",
        "- Use only the information provided in the context documents\n",
        "- These documents were selected using both semantic similarity and keyword relevance\n",
        "- If the information is not sufficient to answer the question, state this clearly\n",
        "- Provide specific details and numbers when available\n",
        "- Structure your answer clearly and concisely\n",
        "- If data spans multiple years or sources, organize it logically\n",
        "\n",
        "ANSWER:\"\"\"\n",
        "\n",
        "    # Send to OpenAI LLM\n",
        "    try:\n",
        "        response = llm.invoke(prompt)\n",
        "        answer = response.content.strip()\n",
        "\n",
        "        print(f\"\\nü§ñ OpenAI LLM Response Generated ({len(answer)} characters)\")\n",
        "        return answer\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error generating answer with OpenAI: {str(e)}\"\n",
        "\n",
        "def evaluate_answers(answer1: str, answer2: str, llm, query: str = None) -> str:\n",
        "    \"\"\"\n",
        "    Evaluate and compare two answers using LLM to determine which is better\n",
        "\n",
        "    Args:\n",
        "        answer1: Answer generated without re-ranking\n",
        "        answer2: Answer generated with Cohere re-ranking\n",
        "        llm: OpenAI LLM instance for evaluation\n",
        "        query: Original query (optional, for context)\n",
        "\n",
        "    Returns:\n",
        "        Detailed comparison and evaluation from the LLM\n",
        "    \"\"\"\n",
        "\n",
        "    # Create evaluation prompt\n",
        "    prompt = f\"\"\"You are an expert evaluator tasked with comparing two AI-generated answers to determine which one is better. Please analyze both answers carefully and provide a detailed comparison.\n",
        "\n",
        "{f\"ORIGINAL QUERY: {query}\" if query else \"\"}\n",
        "\n",
        "ANSWER 1 (Without Re-ranking):\n",
        "{answer1}\n",
        "\n",
        "ANSWER 2 (With Re-ranking):\n",
        "{answer2}\n",
        "\n",
        "EVALUATION CRITERIA:\n",
        "Please evaluate both answers based on the following criteria and provide a detailed analysis:\n",
        "\n",
        "1. **ACCURACY & FACTUAL CORRECTNESS**\n",
        "   - Which answer contains more accurate information?\n",
        "   - Are there any factual errors or inconsistencies?\n",
        "\n",
        "2. **COMPLETENESS & COMPREHENSIVENESS**\n",
        "   - Which answer provides more complete coverage of the topic?\n",
        "   - Does one answer miss important aspects that the other covers?\n",
        "\n",
        "3. **RELEVANCE & FOCUS**\n",
        "   - Which answer stays more focused on the specific question asked?\n",
        "   - Does one contain more irrelevant or tangential information?\n",
        "\n",
        "4. **CLARITY & ORGANIZATION**\n",
        "   - Which answer is clearer and easier to understand?\n",
        "   - How well is the information structured and organized?\n",
        "\n",
        "5. **SPECIFIC DETAILS & EVIDENCE**\n",
        "   - Which answer provides more specific details, numbers, or concrete examples?\n",
        "   - How well does each answer support its claims with evidence?\n",
        "\n",
        "6. **OVERALL QUALITY & USEFULNESS**\n",
        "   - Which answer would be more helpful to someone seeking this information?\n",
        "   - Consider the practical value and actionability of each response.\n",
        "\n",
        "COMPARISON FORMAT:\n",
        "Please structure your evaluation as follows:\n",
        "\n",
        "**WINNER: [Answer 1 / Answer 2 / Tie]**\n",
        "\n",
        "**DETAILED ANALYSIS:**\n",
        "\n",
        "**Accuracy & Factual Correctness:**\n",
        "- Answer 1: [Analysis]\n",
        "- Answer 2: [Analysis]\n",
        "- Winner: [Answer 1/Answer 2/Tie] - [Brief reason]\n",
        "\n",
        "**Completeness & Comprehensiveness:**\n",
        "- Answer 1: [Analysis]\n",
        "- Answer 2: [Analysis]\n",
        "- Winner: [Answer 1/Answer 2/Tie] - [Brief reason]\n",
        "\n",
        "**Relevance & Focus:**\n",
        "- Answer 1: [Analysis]\n",
        "- Answer 2: [Analysis]\n",
        "- Winner: [Answer 1/Answer 2/Tie] - [Brief reason]\n",
        "\n",
        "**Clarity & Organization:**\n",
        "- Answer 1: [Analysis]\n",
        "- Answer 2: [Analysis]\n",
        "- Winner: [Answer 1/Answer 2/Tie] - [Brief reason]\n",
        "\n",
        "**Specific Details & Evidence:**\n",
        "- Answer 1: [Analysis]\n",
        "- Answer 2: [Analysis]\n",
        "- Winner: [Answer 1/Answer 2/Tie] - [Brief reason]\n",
        "\n",
        "**KEY DIFFERENCES:**\n",
        "- [List 3-5 most significant differences between the answers]\n",
        "\n",
        "**FINAL VERDICT:**\n",
        "- Overall Winner: [Answer 1/Answer 2/Tie]\n",
        "- Confidence Level: [High/Medium/Low]\n",
        "- Main Reasons: [2-3 key reasons for the decision]\n",
        "\n",
        "**RECOMMENDATIONS:**\n",
        "- [Suggestions for improving the weaker answer or both answers]\n",
        "\n",
        "Be objective, thorough, and specific in your analysis. Focus on concrete differences rather than general statements.\"\"\"\n",
        "\n",
        "    # Send to LLM for evaluation\n",
        "    try:\n",
        "        response = llm.invoke(prompt)\n",
        "        evaluation = response.content.strip()\n",
        "\n",
        "        print(f\"\\nüîç Answer Evaluation Completed ({len(evaluation)} characters)\")\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"üìä ANSWER COMPARISON EVALUATION\")\n",
        "        print(\"=\"*80)\n",
        "        print(evaluation)\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        return evaluation\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error during answer evaluation: {str(e)}\"\n",
        "\n",
        "# Test the re-ranking function\n",
        "print(\"üß™ Testing re-ranking function...\")\n",
        "test_query = \"Summarize Amazon R&D spending in 2024\"\n",
        "answer = get_rag_answer(test_query, retriever, llm)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéØ BASIC RAG ANSWER:\")\n",
        "print(\"=\"*80)\n",
        "print(answer)\n",
        "print(\"=\"*80)\n",
        "\n",
        "answer_with_rerank = get_rag_answer_with_cohere_rerank(test_query, retriever, llm)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéØ RE-RANKED RAG ANSWER:\")\n",
        "print(\"=\"*80)\n",
        "print(answer_with_rerank)\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Evaluate the answers\n",
        "evaluate_answers(answer, answer_with_rerank, llm, test_query)\n",
        "\n",
        "print(f\"\\n‚úÖ Step 5 Complete: Re-ranking implementation finished!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
