{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🔍 Advanced RAG Techniques\n",
        "\n",
        "In this notebook, we'll go beyond the basics of Retrieval-Augmented Generation (RAG) and explore advanced techniques that significantly improve the quality of generated answers.\n",
        "\n",
        "### 🧠 What we'll build:\n",
        "\n",
        "We'll start by loading 10-K filings from multiple companies — **Amazon**, **Tesla**, **Nvidia**, and **Apple** — and store them in a **vector database**.\n",
        "\n",
        "Then, we'll build a simple RAG pipeline and progressively apply the following advanced retrieval techniques:\n",
        "\n",
        "- 🔄 **Re-ranking**: Reorder retrieved chunks based on relevance to improve answer quality.\n",
        "- 🔗 **Multi-hop Retrieval**: Decompose complex questions and retrieve supporting information across multiple documents.\n",
        "- 🧭 **Hybrid Search**: Combine sparse (keyword-based) and dense (embedding-based) retrieval for better recall.\n",
        "\n",
        "> This notebook gives you a working playground — not just slides — to see how these techniques really perform on real-world financial filings.\n",
        "\n",
        "**Note:** Download the 10-K documents from SEC - https://www.sec.gov/search-filings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📋 Notebook Progress Tracker\n",
        "\n",
        "✅ **Step 1**: Environment Setup & Configuration  \n",
        "⏳ **Step 2**: Document Loading & Chunking  \n",
        "⏳ **Step 3**: Embedding Generation & Storage  \n",
        "⏳ **Step 4**: Basic RAG Implementation  \n",
        "⏳ **Step 5**: Re-ranking with Cohere  \n",
        "⏳ **Step 6**: Multi-Hop Retrieval  \n",
        "⏳ **Step 7**: Hybrid Search (BM25 + Dense)  \n",
        "⏳ **Step 8**: Evaluation & Comparison  \n",
        "\n",
        "---\n",
        "\n",
        "**Current Status**: Setting up environment and dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔧 Step 1: Environment Setup & Configuration\n",
        "\n",
        "First, let's set up our environment variables and import all necessary libraries.\n",
        "\n",
        "**Progress**: Loading environment variables and checking dependencies..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔧 Environment Variables Status:\n",
            "------------------------------\n",
            "✅ PINECONE_API_KEY: Set\n",
            "✅ PINECONE_INDEX: Set\n",
            "✅ PINECONE_URL: Set\n",
            "✅ OPENAI_API_KEY: Set\n",
            "✅ COHERE_API_KEY: Set\n",
            "\n",
            "🎉 All environment variables loaded successfully!\n",
            "📋 Pinecone Index: advance-rag\n",
            "\n",
            "📂 Checking for PDF files in '../data' directory...\n",
            "✅ Found Nvidia.pdf - 4.1 MB\n",
            "✅ Found Tesla.pdf - 8.7 MB\n",
            "✅ Found Apple.pdf - 3.9 MB\n",
            "✅ Found Amazon.pdf - 3.1 MB\n",
            "\n",
            "🎉 All 4 PDF files found successfully!\n",
            "Ready to proceed with document loading and chunking.\n",
            "\n",
            "✅ Step 1 Complete: Environment setup finished!\n"
          ]
        }
      ],
      "source": [
        "# Environment Setup\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import re\n",
        "from typing import Dict, List, Tuple\n",
        "import uuid\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Verify environment variables are loaded\n",
        "required_vars = ['PINECONE_API_KEY', 'PINECONE_INDEX', 'PINECONE_URL', 'OPENAI_API_KEY','COHERE_API_KEY']\n",
        "\n",
        "print(\"🔧 Environment Variables Status:\")\n",
        "print(\"-\" * 30)\n",
        "for var in required_vars:\n",
        "    value = os.getenv(var)\n",
        "    if value:\n",
        "        print(f\"✅ {var}: Set\")\n",
        "    else:\n",
        "        print(f\"❌ {var}: Missing\")\n",
        "\n",
        "# Check if all required variables are present\n",
        "missing_vars = [var for var in required_vars if not os.getenv(var)]\n",
        "\n",
        "if missing_vars:\n",
        "    print(f\"\\n❌ Missing variables: {missing_vars}\")\n",
        "    print(\"Please create a .env file and add all required variables\")\n",
        "else:\n",
        "    print(f\"\\n🎉 All environment variables loaded successfully!\")\n",
        "    print(f\"📋 Pinecone Index: {os.getenv('PINECONE_INDEX')}\")\n",
        "\n",
        "# Setup a data directory for PDFs\n",
        "DATA_DIR = \"../data\"\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    os.makedirs(DATA_DIR)\n",
        "    print(f\"📁 Created directory: {DATA_DIR}\")\n",
        "    print(f\"Please add your PDF files to the '{DATA_DIR}' directory.\")\n",
        "\n",
        "# Verify PDF files in the data directory\n",
        "print(f\"\\n📂 Checking for PDF files in '{DATA_DIR}' directory...\")\n",
        "pdf_files = {}\n",
        "expected_companies = ['Amazon', 'Apple', 'Nvidia', 'Tesla']\n",
        "\n",
        "for filename in os.listdir(DATA_DIR):\n",
        "    if filename.endswith('.pdf'):\n",
        "        # Extract company name from filename\n",
        "        company = filename.replace('.pdf', '').capitalize()\n",
        "        if company in expected_companies:\n",
        "            pdf_files[company.lower()] = os.path.join(DATA_DIR, filename)\n",
        "            file_size = os.path.getsize(os.path.join(DATA_DIR, filename)) / (1024 * 1024)  # Size in MB\n",
        "            print(f\"✅ Found {filename} - {file_size:.1f} MB\")\n",
        "\n",
        "# Check if all expected files are present\n",
        "PDF_FILES = pdf_files\n",
        "total_files = len(PDF_FILES)\n",
        "\n",
        "if total_files == 4:\n",
        "    print(f\"\\n🎉 All {total_files} PDF files found successfully!\")\n",
        "    print(\"Ready to proceed with document loading and chunking.\")\n",
        "else:\n",
        "    print(f\"\\n⚠️  Found {total_files}/4 expected files.\")\n",
        "    print(\"Please make sure Amazon.pdf, Apple.pdf, Nvidia.pdf, and Tesla.pdf are in the 'data' directory.\")\n",
        "\n",
        "print(\"\\n✅ Step 1 Complete: Environment setup finished!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📄 Step 2: Document Loading & Chunking\n",
        "\n",
        "Now we'll load the 10-K documents and split them into manageable chunks with rich metadata.\n",
        "\n",
        "**Progress**: Loading PDFs and creating chunks with metadata..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📚 Loading and chunking PDF documents with enhanced metadata...\n",
            "============================================================\n",
            "\n",
            "📄 Processing NVIDIA: ../data/Nvidia.pdf\n",
            "----------------------------------------\n",
            "   ✅ Loaded 230 pages\n",
            "   ✂️  Created 1461 chunks across 230 pages\n",
            "   📊 Total characters processed: 555,707\n",
            "   📋 Sections detected: Controls and Procedures, Directors and Officers, Business, Risk Factors, General, Executive Compensation, Management Discussion, Financial Statements, Legal Proceedings, Exhibits\n",
            "   ✅ Nvidia: 1461 chunks processed\n",
            "\n",
            "📄 Processing TESLA: ../data/Tesla.pdf\n",
            "----------------------------------------\n",
            "   ✅ Loaded 188 pages\n",
            "   ✂️  Created 1292 chunks across 188 pages\n",
            "   📊 Total characters processed: 499,103\n",
            "   📋 Sections detected: General, Controls and Procedures, Business, Risk Factors, Financial Statements, Directors and Officers, Executive Compensation, Exhibits\n",
            "   ✅ Tesla: 1292 chunks processed\n",
            "\n",
            "📄 Processing APPLE: ../data/Apple.pdf\n",
            "----------------------------------------\n",
            "   ✅ Loaded 123 pages\n",
            "   ✂️  Created 776 chunks across 123 pages\n",
            "   📊 Total characters processed: 300,047\n",
            "   📋 Sections detected: General, Directors and Officers, Business, Legal Proceedings, Risk Factors, Financial Statements, Executive Compensation, Controls and Procedures, Exhibits\n",
            "   ✅ Apple: 776 chunks processed\n",
            "\n",
            "📄 Processing AMAZON: ../data/Amazon.pdf\n",
            "----------------------------------------\n",
            "   ✅ Loaded 138 pages\n",
            "   ✂️  Created 753 chunks across 138 pages\n",
            "   📊 Total characters processed: 285,836\n",
            "   📋 Sections detected: General, Controls and Procedures, Business, Directors and Officers, Risk Factors, Legal Proceedings, Executive Compensation, Financial Statements\n",
            "   ✅ Amazon: 753 chunks processed\n",
            "\n",
            "============================================================\n",
            "📊 ENHANCED PROCESSING SUMMARY\n",
            "============================================================\n",
            "📋 Nvidia: 1,461 chunks\n",
            "   └── Controls and Procedures: 52 chunks\n",
            "   └── Directors and Officers: 111 chunks\n",
            "   └── Business: 546 chunks\n",
            "   └── Risk Factors: 29 chunks\n",
            "   └── General: 330 chunks\n",
            "   └── Executive Compensation: 173 chunks\n",
            "   └── Management Discussion: 7 chunks\n",
            "   └── Financial Statements: 178 chunks\n",
            "   └── Legal Proceedings: 29 chunks\n",
            "   └── Exhibits: 6 chunks\n",
            "📋 Tesla: 1,292 chunks\n",
            "   └── General: 316 chunks\n",
            "   └── Controls and Procedures: 30 chunks\n",
            "   └── Business: 511 chunks\n",
            "   └── Risk Factors: 7 chunks\n",
            "   └── Financial Statements: 255 chunks\n",
            "   └── Directors and Officers: 82 chunks\n",
            "   └── Executive Compensation: 74 chunks\n",
            "   └── Exhibits: 17 chunks\n",
            "📋 Apple: 776 chunks\n",
            "   └── General: 167 chunks\n",
            "   └── Directors and Officers: 46 chunks\n",
            "   └── Business: 239 chunks\n",
            "   └── Legal Proceedings: 51 chunks\n",
            "   └── Risk Factors: 35 chunks\n",
            "   └── Financial Statements: 155 chunks\n",
            "   └── Executive Compensation: 55 chunks\n",
            "   └── Controls and Procedures: 23 chunks\n",
            "   └── Exhibits: 5 chunks\n",
            "📋 Amazon: 753 chunks\n",
            "   └── General: 150 chunks\n",
            "   └── Controls and Procedures: 24 chunks\n",
            "   └── Business: 224 chunks\n",
            "   └── Directors and Officers: 29 chunks\n",
            "   └── Risk Factors: 46 chunks\n",
            "   └── Legal Proceedings: 46 chunks\n",
            "   └── Executive Compensation: 29 chunks\n",
            "   └── Financial Statements: 205 chunks\n",
            "\n",
            "🎯 TOTALS:\n",
            "   📚 Total chunks: 4,282\n",
            "   🏢 Companies processed: 4/4\n",
            "   📄 Average chunks per company: 1070\n",
            "\n",
            "✅ Step 2 Complete: Document loading and chunking finished!\n"
          ]
        }
      ],
      "source": [
        "# Document Loading and Chunking\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# Initialize text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50,\n",
        "    length_function=len\n",
        ")\n",
        "\n",
        "def extract_year_from_filename(filename: str) -> str:\n",
        "    \"\"\"Extract year from filename, default to 2023 if not found.\"\"\"\n",
        "    year_match = re.search(r'20\\d{2}', filename)\n",
        "    return year_match.group() if year_match else \"2023\"\n",
        "\n",
        "def detect_section(text: str, page_num: int = None) -> str:\n",
        "    \"\"\"\n",
        "    Detect 10K section based on text content and common section headers.\n",
        "    Returns the most likely section name.\n",
        "    \"\"\"\n",
        "    text_upper = text.upper()\n",
        "\n",
        "    # Common 10K sections with their typical identifiers\n",
        "    section_patterns = [\n",
        "        (\"Business\", [\"ITEM 1\", \"BUSINESS\", \"OUR BUSINESS\", \"THE BUSINESS\"]),\n",
        "        (\"Risk Factors\", [\"ITEM 1A\", \"RISK FACTORS\", \"RISKS\", \"RISK FACTOR\"]),\n",
        "        (\"Legal Proceedings\", [\"ITEM 3\", \"LEGAL PROCEEDINGS\", \"LITIGATION\"]),\n",
        "        (\"Management Discussion\", [\"ITEM 7\", \"MD&A\", \"MANAGEMENT'S DISCUSSION\", \"MANAGEMENT DISCUSSION\"]),\n",
        "        (\"Financial Statements\", [\"ITEM 8\", \"FINANCIAL STATEMENTS\", \"CONSOLIDATED STATEMENTS\", \"BALANCE SHEET\"]),\n",
        "        (\"Controls and Procedures\", [\"ITEM 9A\", \"CONTROLS AND PROCEDURES\", \"INTERNAL CONTROL\"]),\n",
        "        (\"Directors and Officers\", [\"ITEM 10\", \"DIRECTORS\", \"EXECUTIVE OFFICERS\", \"GOVERNANCE\"]),\n",
        "        (\"Executive Compensation\", [\"ITEM 11\", \"EXECUTIVE COMPENSATION\", \"COMPENSATION\"]),\n",
        "        (\"Security Ownership\", [\"ITEM 12\", \"SECURITY OWNERSHIP\", \"BENEFICIAL OWNERSHIP\"]),\n",
        "        (\"Exhibits\", [\"ITEM 15\", \"EXHIBITS\", \"INDEX TO EXHIBITS\"]),\n",
        "    ]\n",
        "\n",
        "    # Score each section based on keyword matches\n",
        "    section_scores = {}\n",
        "    for section_name, keywords in section_patterns:\n",
        "        score = 0\n",
        "        for keyword in keywords:\n",
        "            if keyword in text_upper:\n",
        "                score += text_upper.count(keyword)\n",
        "        section_scores[section_name] = score\n",
        "\n",
        "    # Return section with highest score, or \"General\" if no clear match\n",
        "    best_section = max(section_scores.items(), key=lambda x: x[1])\n",
        "    return best_section[0] if best_section[1] > 0 else \"General\"\n",
        "\n",
        "def create_chunk_id(company: str, year: str, section: str, chunk_index: int) -> str:\n",
        "    \"\"\"Create a standardized chunk ID.\"\"\"\n",
        "    company_clean = company.lower().replace(\" \", \"_\")\n",
        "    section_clean = section.lower().replace(\" \", \"_\").replace(\"'\", \"\")\n",
        "    return f\"{company_clean}_{year}_{section_clean}_{chunk_index:02d}\"\n",
        "\n",
        "def get_source_doc_id(filename: str) -> str:\n",
        "    \"\"\"Extract clean document ID from filename.\"\"\"\n",
        "    import os\n",
        "    base_name = os.path.basename(filename)\n",
        "    return base_name\n",
        "\n",
        "def process_company_documents(company: str, filename: str) -> List[Document]:\n",
        "    \"\"\"Process a single company's 10K document with enhanced metadata.\"\"\"\n",
        "    print(f\"\\n📄 Processing {company.upper()}: {filename}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    try:\n",
        "        # Load PDF using PyMuPDFLoader\n",
        "        loader = PyMuPDFLoader(filename)\n",
        "        documents = loader.load()\n",
        "        print(f\"   ✅ Loaded {len(documents)} pages\")\n",
        "\n",
        "        # Extract metadata\n",
        "        year = \"2024\"\n",
        "        source_doc_id = get_source_doc_id(filename)\n",
        "\n",
        "        company_chunks = []\n",
        "        chunk_index = 0\n",
        "\n",
        "        # Process each page separately to maintain page number tracking\n",
        "        for page_num, doc in enumerate(documents, 1):\n",
        "            page_content = doc.page_content\n",
        "            page_chars = len(page_content)\n",
        "\n",
        "            if page_chars < 50:  # Skip very short pages\n",
        "                continue\n",
        "\n",
        "            # Detect section for this page\n",
        "            section = detect_section(page_content, page_num)\n",
        "\n",
        "            # Split page into chunks\n",
        "            page_chunks = text_splitter.split_text(page_content)\n",
        "\n",
        "            # Create Document objects for each chunk\n",
        "            for chunk_text in page_chunks:\n",
        "                chunk_id = create_chunk_id(company, year, section, chunk_index)\n",
        "\n",
        "                chunk_doc = Document(\n",
        "                    page_content=chunk_text,\n",
        "                    metadata={\n",
        "                        \"company\": company,\n",
        "                        \"year\": int(year),\n",
        "                        \"section\": section,\n",
        "                        \"chunk_id\": chunk_id,\n",
        "                        \"source_doc_id\": source_doc_id,\n",
        "                        \"page_number\": page_num,\n",
        "                        \"chunk_text\": chunk_text,\n",
        "                        \"chunk_index\": chunk_index,\n",
        "                        \"chunk_size\": len(chunk_text),\n",
        "                        \"source_file\": filename\n",
        "                    }\n",
        "                )\n",
        "\n",
        "                company_chunks.append(chunk_doc)\n",
        "                chunk_index += 1\n",
        "\n",
        "        print(f\"   ✂️  Created {len(company_chunks)} chunks across {len(documents)} pages\")\n",
        "        print(f\"   📊 Total characters processed: {sum(len(doc.page_content) for doc in documents):,}\")\n",
        "\n",
        "        # Section summary\n",
        "        sections_found = {}\n",
        "        for chunk in company_chunks:\n",
        "            section = chunk.metadata['section']\n",
        "            sections_found[section] = sections_found.get(section, 0) + 1\n",
        "\n",
        "        print(f\"   📋 Sections detected: {', '.join(sections_found.keys())}\")\n",
        "\n",
        "        return company_chunks\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   ❌ Error processing {filename}: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Main processing loop\n",
        "all_documents = []\n",
        "chunk_counts = {}\n",
        "section_breakdown = {}\n",
        "\n",
        "print(\"📚 Loading and chunking PDF documents with enhanced metadata...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for company, filename in PDF_FILES.items():\n",
        "    company_chunks = process_company_documents(company, filename)\n",
        "\n",
        "    if company_chunks:\n",
        "        all_documents.extend(company_chunks)\n",
        "        chunk_counts[company] = len(company_chunks)\n",
        "\n",
        "        # Track sections per company\n",
        "        company_sections = {}\n",
        "        for chunk in company_chunks:\n",
        "            section = chunk.metadata['section']\n",
        "            company_sections[section] = company_sections.get(section, 0) + 1\n",
        "        section_breakdown[company] = company_sections\n",
        "\n",
        "        print(f\"   ✅ {company.capitalize()}: {len(company_chunks)} chunks processed\")\n",
        "    else:\n",
        "        chunk_counts[company] = 0\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"📊 ENHANCED PROCESSING SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Print chunks per company\n",
        "for company, count in chunk_counts.items():\n",
        "    print(f\"📋 {company.capitalize()}: {count:,} chunks\")\n",
        "    if company in section_breakdown:\n",
        "        for section, section_count in section_breakdown[company].items():\n",
        "            print(f\"   └── {section}: {section_count} chunks\")\n",
        "\n",
        "# Overall summary\n",
        "total_chunks = len(all_documents)\n",
        "total_companies = len([c for c in chunk_counts.values() if c > 0])\n",
        "\n",
        "print(f\"\\n🎯 TOTALS:\")\n",
        "print(f\"   📚 Total chunks: {total_chunks:,}\")\n",
        "print(f\"   🏢 Companies processed: {total_companies}/{len(PDF_FILES)}\")\n",
        "if total_companies > 0:\n",
        "    print(f\"   📄 Average chunks per company: {total_chunks/total_companies:.0f}\")\n",
        "\n",
        "print(f\"\\n✅ Step 2 Complete: Document loading and chunking finished!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🤖 Step 3: Embedding Generation & Storage\n",
        "\n",
        "Now we'll generate embeddings for all document chunks and store them in Pinecone vector database.\n",
        "\n",
        "**Progress**: Loading embedding model and storing vectors in Pinecone..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/anidhula/.pyenv/versions/3.12.2/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🤖 Loading multilingual-e5-large model...\n",
            "✅ Model loaded successfully\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
            "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
            "0.00s - to python to disable frozen modules.\n",
            "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
            "/Users/anidhula/.pyenv/versions/3.12.2/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📊 Embedding dimensions: 1024\n",
            "\n",
            "🔗 Connecting to Pinecone...\n",
            "✅ Connected to index: advance-rag\n",
            "\n",
            "🚀 Generating embeddings and storing in Pinecone...\n",
            "============================================================\n",
            "\n",
            "📦 Processing batch 1/43\n",
            "   📄 Documents 1-100 of 4282\n",
            "   🤖 Generating embeddings...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/anidhula/.pyenv/versions/3.12.2/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   📤 Uploading to Pinecone...\n",
            "   ✅ Batch stored successfully (100 vectors)\n",
            "\n",
            "📦 Processing batch 2/43\n",
            "   📄 Documents 101-200 of 4282\n",
            "   🤖 Generating embeddings...\n",
            "   📤 Uploading to Pinecone...\n",
            "   ✅ Batch stored successfully (100 vectors)\n",
            "\n",
            "📦 Processing batch 3/43\n",
            "   📄 Documents 201-300 of 4282\n",
            "   🤖 Generating embeddings...\n",
            "   📤 Uploading to Pinecone...\n",
            "   ✅ Batch stored successfully (100 vectors)\n",
            "\n",
            "📦 Processing batch 4/43\n",
            "   📄 Documents 301-400 of 4282\n",
            "   🤖 Generating embeddings...\n",
            "   📤 Uploading to Pinecone...\n",
            "   ✅ Batch stored successfully (100 vectors)\n",
            "\n",
            "📦 Processing batch 5/43\n",
            "   📄 Documents 401-500 of 4282\n",
            "   🤖 Generating embeddings...\n",
            "   📤 Uploading to Pinecone...\n",
            "   ✅ Batch stored successfully (100 vectors)\n",
            "\n",
            "📦 Processing batch 6/43\n",
            "   📄 Documents 501-600 of 4282\n",
            "   🤖 Generating embeddings...\n",
            "   📤 Uploading to Pinecone...\n",
            "   ✅ Batch stored successfully (100 vectors)\n",
            "\n",
            "📦 Processing batch 7/43\n",
            "   📄 Documents 601-700 of 4282\n",
            "   🤖 Generating embeddings...\n",
            "   📤 Uploading to Pinecone...\n",
            "   ✅ Batch stored successfully (100 vectors)\n",
            "\n",
            "📦 Processing batch 8/43\n",
            "   📄 Documents 701-800 of 4282\n",
            "   🤖 Generating embeddings...\n",
            "   📤 Uploading to Pinecone...\n",
            "   ✅ Batch stored successfully (100 vectors)\n",
            "\n",
            "📦 Processing batch 9/43\n",
            "   📄 Documents 801-900 of 4282\n",
            "   🤖 Generating embeddings...\n",
            "   📤 Uploading to Pinecone...\n",
            "   ✅ Batch stored successfully (100 vectors)\n",
            "\n",
            "📦 Processing batch 10/43\n",
            "   📄 Documents 901-1000 of 4282\n",
            "   🤖 Generating embeddings...\n",
            "   📤 Uploading to Pinecone...\n",
            "   ✅ Batch stored successfully (100 vectors)\n",
            "\n",
            "📦 Processing batch 11/43\n",
            "   📄 Documents 1001-1100 of 4282\n",
            "   🤖 Generating embeddings...\n",
            "   📤 Uploading to Pinecone...\n",
            "   ✅ Batch stored successfully (100 vectors)\n",
            "\n",
            "📦 Processing batch 12/43\n",
            "   📄 Documents 1101-1200 of 4282\n",
            "   🤖 Generating embeddings...\n",
            "   📤 Uploading to Pinecone...\n",
            "   ✅ Batch stored successfully (100 vectors)\n",
            "\n",
            "📦 Processing batch 13/43\n",
            "   📄 Documents 1201-1300 of 4282\n",
            "   🤖 Generating embeddings...\n",
            "   📤 Uploading to Pinecone...\n",
            "   ✅ Batch stored successfully (100 vectors)\n",
            "\n",
            "📦 Processing batch 14/43\n",
            "   📄 Documents 1301-1400 of 4282\n",
            "   🤖 Generating embeddings...\n",
            "   📤 Uploading to Pinecone...\n",
            "   ✅ Batch stored successfully (100 vectors)\n",
            "\n",
            "📦 Processing batch 15/43\n",
            "   📄 Documents 1401-1500 of 4282\n",
            "   🤖 Generating embeddings...\n",
            "   📤 Uploading to Pinecone...\n",
            "   ✅ Batch stored successfully (100 vectors)\n",
            "\n",
            "📦 Processing batch 16/43\n",
            "   📄 Documents 1501-1600 of 4282\n",
            "   🤖 Generating embeddings...\n",
            "   📤 Uploading to Pinecone...\n",
            "   ✅ Batch stored successfully (100 vectors)\n",
            "\n",
            "📦 Processing batch 17/43\n",
            "   📄 Documents 1601-1700 of 4282\n",
            "   🤖 Generating embeddings...\n",
            "   📤 Uploading to Pinecone...\n",
            "   ✅ Batch stored successfully (100 vectors)\n",
            "\n",
            "📦 Processing batch 18/43\n",
            "   📄 Documents 1701-1800 of 4282\n",
            "   🤖 Generating embeddings...\n",
            "   📤 Uploading to Pinecone...\n",
            "   ✅ Batch stored successfully (100 vectors)\n",
            "\n",
            "📦 Processing batch 19/43\n",
            "   📄 Documents 1801-1900 of 4282\n",
            "   🤖 Generating embeddings...\n",
            "   📤 Uploading to Pinecone...\n",
            "   ✅ Batch stored successfully (100 vectors)\n",
            "\n",
            "📦 Processing batch 20/43\n",
            "   📄 Documents 1901-2000 of 4282\n",
            "   🤖 Generating embeddings...\n",
            "   📤 Uploading to Pinecone...\n",
            "   ✅ Batch stored successfully (100 vectors)\n",
            "\n",
            "📦 Processing batch 21/43\n",
            "   📄 Documents 2001-2100 of 4282\n",
            "   🤖 Generating embeddings...\n",
            "   📤 Uploading to Pinecone...\n",
            "   ✅ Batch stored successfully (100 vectors)\n",
            "\n",
            "📦 Processing batch 22/43\n",
            "   📄 Documents 2101-2200 of 4282\n",
            "   🤖 Generating embeddings...\n",
            "   📤 Uploading to Pinecone...\n",
            "   ✅ Batch stored successfully (100 vectors)\n",
            "\n",
            "📦 Processing batch 23/43\n",
            "   📄 Documents 2201-2300 of 4282\n",
            "   🤖 Generating embeddings...\n",
            "   📤 Uploading to Pinecone...\n",
            "   ✅ Batch stored successfully (100 vectors)\n",
            "\n",
            "📦 Processing batch 24/43\n",
            "   📄 Documents 2301-2400 of 4282\n",
            "   🤖 Generating embeddings...\n",
            "   📤 Uploading to Pinecone...\n",
            "   ✅ Batch stored successfully (100 vectors)\n",
            "\n",
            "📦 Processing batch 25/43\n",
            "   📄 Documents 2401-2500 of 4282\n",
            "   🤖 Generating embeddings...\n",
            "   📤 Uploading to Pinecone...\n",
            "   ✅ Batch stored successfully (100 vectors)\n",
            "\n",
            "📦 Processing batch 26/43\n",
            "   📄 Documents 2501-2600 of 4282\n",
            "   🤖 Generating embeddings...\n",
            "   📤 Uploading to Pinecone...\n",
            "   ✅ Batch stored successfully (100 vectors)\n",
            "\n",
            "📦 Processing batch 27/43\n",
            "   📄 Documents 2601-2700 of 4282\n",
            "   🤖 Generating embeddings...\n",
            "   📤 Uploading to Pinecone...\n",
            "   ✅ Batch stored successfully (100 vectors)\n",
            "\n",
            "📦 Processing batch 28/43\n",
            "   📄 Documents 2701-2800 of 4282\n",
            "   🤖 Generating embeddings...\n",
            "   📤 Uploading to Pinecone...\n",
            "   ✅ Batch stored successfully (100 vectors)\n",
            "\n",
            "📦 Processing batch 29/43\n",
            "   📄 Documents 2801-2900 of 4282\n",
            "   🤖 Generating embeddings...\n",
            "   📤 Uploading to Pinecone...\n",
            "   ✅ Batch stored successfully (100 vectors)\n",
            "\n",
            "📦 Processing batch 30/43\n",
            "   📄 Documents 2901-3000 of 4282\n",
            "   🤖 Generating embeddings...\n",
            "   📤 Uploading to Pinecone...\n",
            "   ✅ Batch stored successfully (100 vectors)\n",
            "\n",
            "📦 Processing batch 31/43\n",
            "   📄 Documents 3001-3100 of 4282\n",
            "   🤖 Generating embeddings...\n",
            "   📤 Uploading to Pinecone...\n",
            "   ✅ Batch stored successfully (100 vectors)\n",
            "\n",
            "📦 Processing batch 32/43\n",
            "   📄 Documents 3101-3200 of 4282\n",
            "   🤖 Generating embeddings...\n",
            "   📤 Uploading to Pinecone...\n",
            "   ✅ Batch stored successfully (100 vectors)\n",
            "\n",
            "📦 Processing batch 33/43\n",
            "   📄 Documents 3201-3300 of 4282\n",
            "   🤖 Generating embeddings...\n",
            "   📤 Uploading to Pinecone...\n",
            "   ✅ Batch stored successfully (100 vectors)\n",
            "\n",
            "📦 Processing batch 34/43\n",
            "   📄 Documents 3301-3400 of 4282\n",
            "   🤖 Generating embeddings...\n",
            "   📤 Uploading to Pinecone...\n",
            "   ✅ Batch stored successfully (100 vectors)\n",
            "\n",
            "📦 Processing batch 35/43\n",
            "   📄 Documents 3401-3500 of 4282\n",
            "   🤖 Generating embeddings...\n",
            "   📤 Uploading to Pinecone...\n",
            "   ✅ Batch stored successfully (100 vectors)\n",
            "\n",
            "📦 Processing batch 36/43\n",
            "   📄 Documents 3501-3600 of 4282\n",
            "   🤖 Generating embeddings...\n",
            "   📤 Uploading to Pinecone...\n",
            "   ✅ Batch stored successfully (100 vectors)\n",
            "\n",
            "📦 Processing batch 37/43\n",
            "   📄 Documents 3601-3700 of 4282\n",
            "   🤖 Generating embeddings...\n",
            "   📤 Uploading to Pinecone...\n",
            "   ✅ Batch stored successfully (100 vectors)\n",
            "\n",
            "📦 Processing batch 38/43\n",
            "   📄 Documents 3701-3800 of 4282\n",
            "   🤖 Generating embeddings...\n",
            "   📤 Uploading to Pinecone...\n",
            "   ✅ Batch stored successfully (100 vectors)\n",
            "\n",
            "📦 Processing batch 39/43\n",
            "   📄 Documents 3801-3900 of 4282\n",
            "   🤖 Generating embeddings...\n",
            "   📤 Uploading to Pinecone...\n",
            "   ✅ Batch stored successfully (100 vectors)\n",
            "\n",
            "📦 Processing batch 40/43\n",
            "   📄 Documents 3901-4000 of 4282\n",
            "   🤖 Generating embeddings...\n",
            "   📤 Uploading to Pinecone...\n",
            "   ✅ Batch stored successfully (100 vectors)\n",
            "\n",
            "📦 Processing batch 41/43\n",
            "   📄 Documents 4001-4100 of 4282\n",
            "   🤖 Generating embeddings...\n",
            "   📤 Uploading to Pinecone...\n",
            "   ✅ Batch stored successfully (100 vectors)\n",
            "\n",
            "📦 Processing batch 42/43\n",
            "   📄 Documents 4101-4200 of 4282\n",
            "   🤖 Generating embeddings...\n",
            "   📤 Uploading to Pinecone...\n",
            "   ✅ Batch stored successfully (100 vectors)\n",
            "\n",
            "📦 Processing batch 43/43\n",
            "   📄 Documents 4201-4282 of 4282\n",
            "   🤖 Generating embeddings...\n",
            "   📤 Uploading to Pinecone...\n",
            "   ✅ Batch stored successfully (82 vectors)\n",
            "\n",
            "============================================================\n",
            "🎯 EMBEDDING & STORAGE SUMMARY\n",
            "============================================================\n",
            "📋 Nvidia: 1,461 vectors stored\n",
            "📋 Tesla: 1,292 vectors stored\n",
            "📋 Apple: 776 vectors stored\n",
            "📋 Amazon: 753 vectors stored\n",
            "\n",
            "📊 TOTALS:\n",
            "   🗄️  Total vectors stored: 4,282\n",
            "   🏢 Companies: 4\n",
            "   📐 Embedding dimensions: 1024\n",
            "   🤖 Model: intfloat/multilingual-e5-large\n",
            "\n",
            "🔍 Verifying Pinecone index...\n",
            "   📈 Total vectors in index: 15946\n",
            "   📁 Namespaces: ['']\n",
            "\n",
            "🎉 SUCCESS! All 4282 document chunks embedded and stored!\n",
            "✅ Ready for RAG querying!\n",
            "\n",
            "✅ Step 3 Complete: Embedding generation and storage finished!\n"
          ]
        }
      ],
      "source": [
        "# Embedding Generation and Storage\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pinecone import Pinecone\n",
        "\n",
        "# Initialize embedding model\n",
        "print(\"🤖 Loading multilingual-e5-large model...\")\n",
        "model = SentenceTransformer('intfloat/multilingual-e5-large')\n",
        "print(\"✅ Model loaded successfully\")\n",
        "\n",
        "# Test embedding to verify dimensions\n",
        "test_embedding = model.encode(\"test\", normalize_embeddings=True)\n",
        "print(f\"📊 Embedding dimensions: {len(test_embedding)}\")\n",
        "\n",
        "# Initialize Pinecone\n",
        "print(\"\\n🔗 Connecting to Pinecone...\")\n",
        "pc = Pinecone(api_key=os.getenv('PINECONE_API_KEY'))\n",
        "index_name = os.getenv('PINECONE_INDEX')\n",
        "\n",
        "# Check if the index exists, create if it doesn't\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    print(f\"⚠️ Index '{index_name}' not found. Please create it in your Pinecone project.\")\n",
        "    # Example of how to create an index (adjust dimension as needed)\n",
        "    # from pinecone import ServerlessSpec\n",
        "    # pc.create_index(\n",
        "    #     name=index_name,\n",
        "    #     dimension=len(test_embedding),\n",
        "    #     metric=\"cosine\",\n",
        "    #     spec=ServerlessSpec(\n",
        "    #         cloud='aws',\n",
        "    #         region='us-west-2'\n",
        "    #     )\n",
        "    # )\n",
        "    # print(f\"✅ Created index: {index_name}\")\n",
        "\n",
        "index = pc.Index(index_name)\n",
        "print(f\"✅ Connected to index: {index_name}\")\n",
        "\n",
        "# Generate embeddings and store in Pinecone\n",
        "print(\"\\n🚀 Generating embeddings and storing in Pinecone...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "batch_size = 100  # Process in batches\n",
        "total_stored = 0\n",
        "company_stored = {}\n",
        "\n",
        "for i in range(0, len(all_documents), batch_size):\n",
        "    batch_docs = all_documents[i:i + batch_size]\n",
        "\n",
        "    print(f\"\\n📦 Processing batch {i//batch_size + 1}/{(len(all_documents)-1)//batch_size + 1}\")\n",
        "    print(f\"   📄 Documents {i+1}-{min(i+batch_size, len(all_documents))} of {len(all_documents)}\")\n",
        "\n",
        "    # Extract texts from batch\n",
        "    texts = [doc.page_content for doc in batch_docs]\n",
        "\n",
        "    # Generate embeddings\n",
        "    print(\"   🤖 Generating embeddings...\")\n",
        "    embeddings = model.encode(texts, normalize_embeddings=True)\n",
        "\n",
        "    # Prepare vectors for Pinecone\n",
        "    vectors = []\n",
        "    for doc, embedding in zip(batch_docs, embeddings):\n",
        "        vector_id = str(uuid.uuid4())\n",
        "\n",
        "        # Prepare metadata with requested fields\n",
        "        metadata = {\n",
        "            'company': doc.metadata['company'],\n",
        "            'year': doc.metadata['year'],\n",
        "            'section': doc.metadata.get('section', 'Financial Statements'),\n",
        "            'chunk_id': f\"{doc.metadata['company'].lower().replace(' (1)', '')}_{doc.metadata['year']}_financial_statements_{doc.metadata.get('chunk_id', str(i).zfill(2))}\",\n",
        "            'source_doc_id': doc.metadata['source_file'],\n",
        "            'page_number': doc.metadata.get('page_number', 1),\n",
        "            'chunk_size': f\"{len(doc.page_content)} characters\",\n",
        "            'source': doc.metadata['source_file'],\n",
        "            'chunk_text': doc.page_content\n",
        "        }\n",
        "\n",
        "        vector = {\n",
        "            'id': vector_id,\n",
        "            'values': embedding.tolist(),\n",
        "            'metadata': metadata\n",
        "        }\n",
        "        vectors.append(vector)\n",
        "\n",
        "    # Store in Pinecone\n",
        "    print(\"   📤 Uploading to Pinecone...\")\n",
        "    try:\n",
        "        index.upsert(vectors=vectors)\n",
        "\n",
        "        # Count by company\n",
        "        for doc in batch_docs:\n",
        "            company = doc.metadata['company']\n",
        "            company_stored[company] = company_stored.get(company, 0) + 1\n",
        "\n",
        "        total_stored += len(vectors)\n",
        "        print(f\"   ✅ Batch stored successfully ({len(vectors)} vectors)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   ❌ Error storing batch: {str(e)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"🎯 EMBEDDING & STORAGE SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Print storage by company\n",
        "for company, count in company_stored.items():\n",
        "    print(f\"📋 {company.capitalize()}: {count:,} vectors stored\")\n",
        "\n",
        "print(f\"\\n📊 TOTALS:\")\n",
        "print(f\"   🗄️  Total vectors stored: {total_stored:,}\")\n",
        "print(f\"   🏢 Companies: {len(company_stored)}\")\n",
        "print(f\"   📐 Embedding dimensions: {len(test_embedding)}\")\n",
        "print(f\"   🤖 Model: intfloat/multilingual-e5-large\")\n",
        "\n",
        "# Verify index stats\n",
        "try:\n",
        "    print(f\"\\n🔍 Verifying Pinecone index...\")\n",
        "    stats = index.describe_index_stats()\n",
        "    print(f\"   📈 Total vectors in index: {stats.total_vector_count}\")\n",
        "    if hasattr(stats, 'namespaces') and stats.namespaces:\n",
        "        print(f\"   📁 Namespaces: {list(stats.namespaces.keys())}\")\n",
        "except Exception as e:\n",
        "    print(f\"   ⚠️  Could not retrieve index stats: {str(e)}\")\n",
        "\n",
        "if total_stored == len(all_documents):\n",
        "    print(f\"\\n🎉 SUCCESS! All {total_stored} document chunks embedded and stored!\")\n",
        "    print(\"✅ Ready for RAG querying!\")\n",
        "else:\n",
        "    print(f\"\\n⚠️  Stored {total_stored}/{len(all_documents)} chunks\")\n",
        "    print(\"Some chunks may have failed to store.\")\n",
        "\n",
        "print(f\"\\n✅ Step 3 Complete: Embedding generation and storage finished!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
