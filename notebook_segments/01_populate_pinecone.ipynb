{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üîç Advanced RAG Techniques\n",
        "\n",
        "In this notebook, we'll go beyond the basics of Retrieval-Augmented Generation (RAG) and explore advanced techniques that significantly improve the quality of generated answers.\n",
        "\n",
        "### üß† What we'll build:\n",
        "\n",
        "We'll start by loading 10-K filings from multiple companies ‚Äî **Amazon**, **Tesla**, **Nvidia**, and **Apple** ‚Äî and store them in a **vector database**.\n",
        "\n",
        "Then, we'll build a simple RAG pipeline and progressively apply the following advanced retrieval techniques:\n",
        "\n",
        "- üîÑ **Re-ranking**: Reorder retrieved chunks based on relevance to improve answer quality.\n",
        "- üîó **Multi-hop Retrieval**: Decompose complex questions and retrieve supporting information across multiple documents.\n",
        "- üß≠ **Hybrid Search**: Combine sparse (keyword-based) and dense (embedding-based) retrieval for better recall.\n",
        "\n",
        "> This notebook gives you a working playground ‚Äî not just slides ‚Äî to see how these techniques really perform on real-world financial filings.\n",
        "\n",
        "**Note:** Download the 10-K documents from SEC - https://www.sec.gov/search-filings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìã Notebook Progress Tracker\n",
        "\n",
        "‚úÖ **Step 1**: Environment Setup & Configuration  \n",
        "‚è≥ **Step 2**: Document Loading & Chunking  \n",
        "‚è≥ **Step 3**: Embedding Generation & Storage  \n",
        "‚è≥ **Step 4**: Basic RAG Implementation  \n",
        "‚è≥ **Step 5**: Re-ranking with Cohere  \n",
        "‚è≥ **Step 6**: Multi-Hop Retrieval  \n",
        "‚è≥ **Step 7**: Hybrid Search (BM25 + Dense)  \n",
        "‚è≥ **Step 8**: Evaluation & Comparison  \n",
        "\n",
        "---\n",
        "\n",
        "**Current Status**: Setting up environment and dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Step 1: Environment Setup & Configuration\n",
        "\n",
        "First, let's set up our environment variables and import all necessary libraries.\n",
        "\n",
        "**Progress**: Loading environment variables and checking dependencies..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Environment Variables Status:\n",
            "------------------------------\n",
            "‚úÖ PINECONE_API_KEY: Set\n",
            "‚úÖ PINECONE_INDEX: Set\n",
            "‚úÖ PINECONE_URL: Set\n",
            "‚úÖ OPENAI_API_KEY: Set\n",
            "‚úÖ COHERE_API_KEY: Set\n",
            "\n",
            "üéâ All environment variables loaded successfully!\n",
            "üìã Pinecone Index: advance-rag\n",
            "\n",
            "üìÇ Checking for PDF files in 'data' directory...\n",
            "‚úÖ Found Nvidia.pdf - 4.1 MB\n",
            "‚úÖ Found Tesla.pdf - 8.7 MB\n",
            "‚úÖ Found Apple.pdf - 3.9 MB\n",
            "‚úÖ Found Amazon.pdf - 3.1 MB\n",
            "\n",
            "üéâ All 4 PDF files found successfully!\n",
            "Ready to proceed with document loading and chunking.\n",
            "\n",
            "‚úÖ Step 1 Complete: Environment setup finished!\n"
          ]
        }
      ],
      "source": [
        "# Environment Setup\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import re\n",
        "from typing import Dict, List, Tuple\n",
        "import uuid\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Verify environment variables are loaded\n",
        "required_vars = ['PINECONE_API_KEY', 'PINECONE_INDEX', 'PINECONE_URL', 'OPENAI_API_KEY','COHERE_API_KEY']\n",
        "\n",
        "print(\"üîß Environment Variables Status:\")\n",
        "print(\"-\" * 30)\n",
        "for var in required_vars:\n",
        "    value = os.getenv(var)\n",
        "    if value:\n",
        "        print(f\"‚úÖ {var}: Set\")\n",
        "    else:\n",
        "        print(f\"‚ùå {var}: Missing\")\n",
        "\n",
        "# Check if all required variables are present\n",
        "missing_vars = [var for var in required_vars if not os.getenv(var)]\n",
        "\n",
        "if missing_vars:\n",
        "    print(f\"\\n‚ùå Missing variables: {missing_vars}\")\n",
        "    print(\"Please create a .env file and add all required variables\")\n",
        "else:\n",
        "    print(f\"\\nüéâ All environment variables loaded successfully!\")\n",
        "    print(f\"üìã Pinecone Index: {os.getenv('PINECONE_INDEX')}\")\n",
        "\n",
        "# Setup a data directory for PDFs\n",
        "DATA_DIR = \"data\"\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    os.makedirs(DATA_DIR)\n",
        "    print(f\"üìÅ Created directory: {DATA_DIR}\")\n",
        "    print(f\"Please add your PDF files to the '{DATA_DIR}' directory.\")\n",
        "\n",
        "# Verify PDF files in the data directory\n",
        "print(f\"\\nüìÇ Checking for PDF files in '{DATA_DIR}' directory...\")\n",
        "pdf_files = {}\n",
        "expected_companies = ['Amazon', 'Apple', 'Nvidia', 'Tesla']\n",
        "\n",
        "for filename in os.listdir(DATA_DIR):\n",
        "    if filename.endswith('.pdf'):\n",
        "        # Extract company name from filename\n",
        "        company = filename.replace('.pdf', '').capitalize()\n",
        "        if company in expected_companies:\n",
        "            pdf_files[company.lower()] = os.path.join(DATA_DIR, filename)\n",
        "            file_size = os.path.getsize(os.path.join(DATA_DIR, filename)) / (1024 * 1024)  # Size in MB\n",
        "            print(f\"‚úÖ Found {filename} - {file_size:.1f} MB\")\n",
        "\n",
        "# Check if all expected files are present\n",
        "PDF_FILES = pdf_files\n",
        "total_files = len(PDF_FILES)\n",
        "\n",
        "if total_files == 4:\n",
        "    print(f\"\\nüéâ All {total_files} PDF files found successfully!\")\n",
        "    print(\"Ready to proceed with document loading and chunking.\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è  Found {total_files}/4 expected files.\")\n",
        "    print(\"Please make sure Amazon.pdf, Apple.pdf, Nvidia.pdf, and Tesla.pdf are in the 'data' directory.\")\n",
        "\n",
        "print(\"\\n‚úÖ Step 1 Complete: Environment setup finished!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÑ Step 2: Document Loading & Chunking\n",
        "\n",
        "Now we'll load the 10-K documents and split them into manageable chunks with rich metadata.\n",
        "\n",
        "**Progress**: Loading PDFs and creating chunks with metadata..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Document Loading and Chunking\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# Initialize text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50,\n",
        "    length_function=len\n",
        ")\n",
        "\n",
        "def extract_year_from_filename(filename: str) -> str:\n",
        "    \"\"\"Extract year from filename, default to 2023 if not found.\"\"\"\n",
        "    year_match = re.search(r'20\\d{2}', filename)\n",
        "    return year_match.group() if year_match else \"2023\"\n",
        "\n",
        "def detect_section(text: str, page_num: int = None) -> str:\n",
        "    \"\"\"\n",
        "    Detect 10K section based on text content and common section headers.\n",
        "    Returns the most likely section name.\n",
        "    \"\"\"\n",
        "    text_upper = text.upper()\n",
        "\n",
        "    # Common 10K sections with their typical identifiers\n",
        "    section_patterns = [\n",
        "        (\"Business\", [\"ITEM 1\", \"BUSINESS\", \"OUR BUSINESS\", \"THE BUSINESS\"]),\n",
        "        (\"Risk Factors\", [\"ITEM 1A\", \"RISK FACTORS\", \"RISKS\", \"RISK FACTOR\"]),\n",
        "        (\"Legal Proceedings\", [\"ITEM 3\", \"LEGAL PROCEEDINGS\", \"LITIGATION\"]),\n",
        "        (\"Management Discussion\", [\"ITEM 7\", \"MD&A\", \"MANAGEMENT'S DISCUSSION\", \"MANAGEMENT DISCUSSION\"]),\n",
        "        (\"Financial Statements\", [\"ITEM 8\", \"FINANCIAL STATEMENTS\", \"CONSOLIDATED STATEMENTS\", \"BALANCE SHEET\"]),\n",
        "        (\"Controls and Procedures\", [\"ITEM 9A\", \"CONTROLS AND PROCEDURES\", \"INTERNAL CONTROL\"]),\n",
        "        (\"Directors and Officers\", [\"ITEM 10\", \"DIRECTORS\", \"EXECUTIVE OFFICERS\", \"GOVERNANCE\"]),\n",
        "        (\"Executive Compensation\", [\"ITEM 11\", \"EXECUTIVE COMPENSATION\", \"COMPENSATION\"]),\n",
        "        (\"Security Ownership\", [\"ITEM 12\", \"SECURITY OWNERSHIP\", \"BENEFICIAL OWNERSHIP\"]),\n",
        "        (\"Exhibits\", [\"ITEM 15\", \"EXHIBITS\", \"INDEX TO EXHIBITS\"]),\n",
        "    ]\n",
        "\n",
        "    # Score each section based on keyword matches\n",
        "    section_scores = {}\n",
        "    for section_name, keywords in section_patterns:\n",
        "        score = 0\n",
        "        for keyword in keywords:\n",
        "            if keyword in text_upper:\n",
        "                score += text_upper.count(keyword)\n",
        "        section_scores[section_name] = score\n",
        "\n",
        "    # Return section with highest score, or \"General\" if no clear match\n",
        "    best_section = max(section_scores.items(), key=lambda x: x[1])\n",
        "    return best_section[0] if best_section[1] > 0 else \"General\"\n",
        "\n",
        "def create_chunk_id(company: str, year: str, section: str, chunk_index: int) -> str:\n",
        "    \"\"\"Create a standardized chunk ID.\"\"\"\n",
        "    company_clean = company.lower().replace(\" \", \"_\")\n",
        "    section_clean = section.lower().replace(\" \", \"_\").replace(\"'\", \"\")\n",
        "    return f\"{company_clean}_{year}_{section_clean}_{chunk_index:02d}\"\n",
        "\n",
        "def get_source_doc_id(filename: str) -> str:\n",
        "    \"\"\"Extract clean document ID from filename.\"\"\"\n",
        "    import os\n",
        "    base_name = os.path.basename(filename)\n",
        "    return base_name\n",
        "\n",
        "def process_company_documents(company: str, filename: str) -> List[Document]:\n",
        "    \"\"\"Process a single company's 10K document with enhanced metadata.\"\"\"\n",
        "    print(f\"\\nüìÑ Processing {company.upper()}: {filename}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    try:\n",
        "        # Load PDF using PyMuPDFLoader\n",
        "        loader = PyMuPDFLoader(filename)\n",
        "        documents = loader.load()\n",
        "        print(f\"   ‚úÖ Loaded {len(documents)} pages\")\n",
        "\n",
        "        # Extract metadata\n",
        "        year = \"2024\"\n",
        "        source_doc_id = get_source_doc_id(filename)\n",
        "\n",
        "        company_chunks = []\n",
        "        chunk_index = 0\n",
        "\n",
        "        # Process each page separately to maintain page number tracking\n",
        "        for page_num, doc in enumerate(documents, 1):\n",
        "            page_content = doc.page_content\n",
        "            page_chars = len(page_content)\n",
        "\n",
        "            if page_chars < 50:  # Skip very short pages\n",
        "                continue\n",
        "\n",
        "            # Detect section for this page\n",
        "            section = detect_section(page_content, page_num)\n",
        "\n",
        "            # Split page into chunks\n",
        "            page_chunks = text_splitter.split_text(page_content)\n",
        "\n",
        "            # Create Document objects for each chunk\n",
        "            for chunk_text in page_chunks:\n",
        "                chunk_id = create_chunk_id(company, year, section, chunk_index)\n",
        "\n",
        "                chunk_doc = Document(\n",
        "                    page_content=chunk_text,\n",
        "                    metadata={\n",
        "                        \"company\": company,\n",
        "                        \"year\": int(year),\n",
        "                        \"section\": section,\n",
        "                        \"chunk_id\": chunk_id,\n",
        "                        \"source_doc_id\": source_doc_id,\n",
        "                        \"page_number\": page_num,\n",
        "                        \"chunk_text\": chunk_text,\n",
        "                        \"chunk_index\": chunk_index,\n",
        "                        \"chunk_size\": len(chunk_text),\n",
        "                        \"source_file\": filename\n",
        "                    }\n",
        "                )\n",
        "\n",
        "                company_chunks.append(chunk_doc)\n",
        "                chunk_index += 1\n",
        "\n",
        "        print(f\"   ‚úÇÔ∏è  Created {len(company_chunks)} chunks across {len(documents)} pages\")\n",
        "        print(f\"   üìä Total characters processed: {sum(len(doc.page_content) for doc in documents):,}\")\n",
        "\n",
        "        # Section summary\n",
        "        sections_found = {}\n",
        "        for chunk in company_chunks:\n",
        "            section = chunk.metadata['section']\n",
        "            sections_found[section] = sections_found.get(section, 0) + 1\n",
        "\n",
        "        print(f\"   üìã Sections detected: {', '.join(sections_found.keys())}\")\n",
        "\n",
        "        return company_chunks\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error processing {filename}: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Main processing loop\n",
        "all_documents = []\n",
        "chunk_counts = {}\n",
        "section_breakdown = {}\n",
        "\n",
        "print(\"üìö Loading and chunking PDF documents with enhanced metadata...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for company, filename in PDF_FILES.items():\n",
        "    company_chunks = process_company_documents(company, filename)\n",
        "\n",
        "    if company_chunks:\n",
        "        all_documents.extend(company_chunks)\n",
        "        chunk_counts[company] = len(company_chunks)\n",
        "\n",
        "        # Track sections per company\n",
        "        company_sections = {}\n",
        "        for chunk in company_chunks:\n",
        "            section = chunk.metadata['section']\n",
        "            company_sections[section] = company_sections.get(section, 0) + 1\n",
        "        section_breakdown[company] = company_sections\n",
        "\n",
        "        print(f\"   ‚úÖ {company.capitalize()}: {len(company_chunks)} chunks processed\")\n",
        "    else:\n",
        "        chunk_counts[company] = 0\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üìä ENHANCED PROCESSING SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Print chunks per company\n",
        "for company, count in chunk_counts.items():\n",
        "    print(f\"üìã {company.capitalize()}: {count:,} chunks\")\n",
        "    if company in section_breakdown:\n",
        "        for section, section_count in section_breakdown[company].items():\n",
        "            print(f\"   ‚îî‚îÄ‚îÄ {section}: {section_count} chunks\")\n",
        "\n",
        "# Overall summary\n",
        "total_chunks = len(all_documents)\n",
        "total_companies = len([c for c in chunk_counts.values() if c > 0])\n",
        "\n",
        "print(f\"\\nüéØ TOTALS:\")\n",
        "print(f\"   üìö Total chunks: {total_chunks:,}\")\n",
        "print(f\"   üè¢ Companies processed: {total_companies}/{len(PDF_FILES)}\")\n",
        "if total_companies > 0:\n",
        "    print(f\"   üìÑ Average chunks per company: {total_chunks/total_companies:.0f}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Step 2 Complete: Document loading and chunking finished!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§ñ Step 3: Embedding Generation & Storage\n",
        "\n",
        "Now we'll generate embeddings for all document chunks and store them in Pinecone vector database.\n",
        "\n",
        "**Progress**: Loading embedding model and storing vectors in Pinecone..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Embedding Generation and Storage\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pinecone import Pinecone\n",
        "\n",
        "# Initialize embedding model\n",
        "print(\"ü§ñ Loading multilingual-e5-large model...\")\n",
        "model = SentenceTransformer('intfloat/multilingual-e5-large')\n",
        "print(\"‚úÖ Model loaded successfully\")\n",
        "\n",
        "# Test embedding to verify dimensions\n",
        "test_embedding = model.encode(\"test\", normalize_embeddings=True)\n",
        "print(f\"üìä Embedding dimensions: {len(test_embedding)}\")\n",
        "\n",
        "# Initialize Pinecone\n",
        "print(\"\\nüîó Connecting to Pinecone...\")\n",
        "pc = Pinecone(api_key=os.getenv('PINECONE_API_KEY'))\n",
        "index_name = os.getenv('PINECONE_INDEX')\n",
        "\n",
        "# Check if the index exists, create if it doesn't\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    print(f\"‚ö†Ô∏è Index '{index_name}' not found. Please create it in your Pinecone project.\")\n",
        "    # Example of how to create an index (adjust dimension as needed)\n",
        "    # from pinecone import ServerlessSpec\n",
        "    # pc.create_index(\n",
        "    #     name=index_name,\n",
        "    #     dimension=len(test_embedding),\n",
        "    #     metric=\"cosine\",\n",
        "    #     spec=ServerlessSpec(\n",
        "    #         cloud='aws',\n",
        "    #         region='us-west-2'\n",
        "    #     )\n",
        "    # )\n",
        "    # print(f\"‚úÖ Created index: {index_name}\")\n",
        "\n",
        "index = pc.Index(index_name)\n",
        "print(f\"‚úÖ Connected to index: {index_name}\")\n",
        "\n",
        "# Generate embeddings and store in Pinecone\n",
        "print(\"\\nüöÄ Generating embeddings and storing in Pinecone...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "batch_size = 100  # Process in batches\n",
        "total_stored = 0\n",
        "company_stored = {}\n",
        "\n",
        "for i in range(0, len(all_documents), batch_size):\n",
        "    batch_docs = all_documents[i:i + batch_size]\n",
        "\n",
        "    print(f\"\\nüì¶ Processing batch {i//batch_size + 1}/{(len(all_documents)-1)//batch_size + 1}\")\n",
        "    print(f\"   üìÑ Documents {i+1}-{min(i+batch_size, len(all_documents))} of {len(all_documents)}\")\n",
        "\n",
        "    # Extract texts from batch\n",
        "    texts = [doc.page_content for doc in batch_docs]\n",
        "\n",
        "    # Generate embeddings\n",
        "    print(\"   ü§ñ Generating embeddings...\")\n",
        "    embeddings = model.encode(texts, normalize_embeddings=True)\n",
        "\n",
        "    # Prepare vectors for Pinecone\n",
        "    vectors = []\n",
        "    for doc, embedding in zip(batch_docs, embeddings):\n",
        "        vector_id = str(uuid.uuid4())\n",
        "\n",
        "        # Prepare metadata with requested fields\n",
        "        metadata = {\n",
        "            'company': doc.metadata['company'],\n",
        "            'year': doc.metadata['year'],\n",
        "            'section': doc.metadata.get('section', 'Financial Statements'),\n",
        "            'chunk_id': f\"{doc.metadata['company'].lower().replace(' (1)', '')}_{doc.metadata['year']}_financial_statements_{doc.metadata.get('chunk_id', str(i).zfill(2))}\",\n",
        "            'source_doc_id': doc.metadata['source_file'],\n",
        "            'page_number': doc.metadata.get('page_number', 1),\n",
        "            'chunk_size': f\"{len(doc.page_content)} characters\",\n",
        "            'source': doc.metadata['source_file'],\n",
        "            'chunk_text': doc.page_content\n",
        "        }\n",
        "\n",
        "        vector = {\n",
        "            'id': vector_id,\n",
        "            'values': embedding.tolist(),\n",
        "            'metadata': metadata\n",
        "        }\n",
        "        vectors.append(vector)\n",
        "\n",
        "    # Store in Pinecone\n",
        "    print(\"   üì§ Uploading to Pinecone...\")\n",
        "    try:\n",
        "        index.upsert(vectors=vectors)\n",
        "\n",
        "        # Count by company\n",
        "        for doc in batch_docs:\n",
        "            company = doc.metadata['company']\n",
        "            company_stored[company] = company_stored.get(company, 0) + 1\n",
        "\n",
        "        total_stored += len(vectors)\n",
        "        print(f\"   ‚úÖ Batch stored successfully ({len(vectors)} vectors)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error storing batch: {str(e)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üéØ EMBEDDING & STORAGE SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Print storage by company\n",
        "for company, count in company_stored.items():\n",
        "    print(f\"üìã {company.capitalize()}: {count:,} vectors stored\")\n",
        "\n",
        "print(f\"\\nüìä TOTALS:\")\n",
        "print(f\"   üóÑÔ∏è  Total vectors stored: {total_stored:,}\")\n",
        "print(f\"   üè¢ Companies: {len(company_stored)}\")\n",
        "print(f\"   üìê Embedding dimensions: {len(test_embedding)}\")\n",
        "print(f\"   ü§ñ Model: intfloat/multilingual-e5-large\")\n",
        "\n",
        "# Verify index stats\n",
        "try:\n",
        "    print(f\"\\nüîç Verifying Pinecone index...\")\n",
        "    stats = index.describe_index_stats()\n",
        "    print(f\"   üìà Total vectors in index: {stats.total_vector_count}\")\n",
        "    if hasattr(stats, 'namespaces') and stats.namespaces:\n",
        "        print(f\"   üìÅ Namespaces: {list(stats.namespaces.keys())}\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ö†Ô∏è  Could not retrieve index stats: {str(e)}\")\n",
        "\n",
        "if total_stored == len(all_documents):\n",
        "    print(f\"\\nüéâ SUCCESS! All {total_stored} document chunks embedded and stored!\")\n",
        "    print(\"‚úÖ Ready for RAG querying!\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è  Stored {total_stored}/{len(all_documents)} chunks\")\n",
        "    print(\"Some chunks may have failed to store.\")\n",
        "\n",
        "print(f\"\\n‚úÖ Step 3 Complete: Embedding generation and storage finished!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Step 4: Basic RAG Implementation\n",
        "\n",
        "Now we'll set up the basic RAG pipeline with OpenAI LLM and Pinecone retriever.\n",
        "\n",
        "**Progress**: Initializing LLM and creating basic RAG function..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ OpenAI API key loaded successfully\n",
            "‚úÖ OpenAI LLM initialized successfully\n",
            "ü§ñ Model: gpt-4o-mini\n",
            "üå°Ô∏è Temperature: 0.1\n",
            "üì° Streaming: False\n",
            "\n",
            "üß™ Test Response: LLM is working correctly.\n",
            "‚úÖ LLM is ready to use!\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'Pinecone' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPlease check your API key and internet connection.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Initialize Pinecone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m pc = \u001b[43mPinecone\u001b[49m(api_key=os.getenv(\u001b[33m\"\u001b[39m\u001b[33mPINECONE_API_KEY\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     40\u001b[39m index_name = os.getenv(\u001b[33m\"\u001b[39m\u001b[33mPINECONE_INDEX\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     41\u001b[39m index = pc.Index(index_name)\n",
            "\u001b[31mNameError\u001b[39m: name 'Pinecone' is not defined"
          ]
        }
      ],
      "source": [
        "# Basic RAG Implementation\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
        "import numpy as np\n",
        "\n",
        "# Get OpenAI API key\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "if not openai_api_key:\n",
        "    raise ValueError(\"OPENAI_API_KEY not found in environment variables or .env file\")\n",
        "\n",
        "print(\"‚úÖ OpenAI API key loaded successfully\")\n",
        "\n",
        "# Initialize OpenAI LLM\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",  # Using GPT-4o-mini for cost efficiency\n",
        "    openai_api_key=openai_api_key,\n",
        "    temperature=0.1,\n",
        "    streaming=False\n",
        ")\n",
        "\n",
        "print(\"‚úÖ OpenAI LLM initialized successfully\")\n",
        "print(f\"ü§ñ Model: gpt-4o-mini\")\n",
        "print(f\"üå°Ô∏è Temperature: 0.1\")\n",
        "print(f\"üì° Streaming: False\")\n",
        "\n",
        "# Test the LLM with a simple query\n",
        "try:\n",
        "    test_response = llm.invoke(\"Hello! Please respond with 'LLM is working correctly.'\")\n",
        "    print(f\"\\nüß™ Test Response: {test_response.content}\")\n",
        "    print(\"‚úÖ LLM is ready to use!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error testing LLM: {str(e)}\")\n",
        "    print(\"Please check your API key and internet connection.\")\n",
        "\n",
        "# Initialize Pinecone\n",
        "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
        "index_name = os.getenv(\"PINECONE_INDEX\")\n",
        "index = pc.Index(index_name)\n",
        "\n",
        "# Initialize embedding model\n",
        "embedding_model = SentenceTransformerEmbeddings(\n",
        "    model_name='intfloat/multilingual-e5-large'\n",
        ")\n",
        "\n",
        "# Create VectorStore\n",
        "vectorstore = PineconeVectorStore(\n",
        "    index=index,\n",
        "    embedding=embedding_model\n",
        ")\n",
        "\n",
        "# Create retriever with similarity search and k=5\n",
        "retriever = vectorstore.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 5}\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Setup complete! Pinecone retriever ready.\")\n",
        "print(f\"üìä Index name: {index_name}\")\n",
        "print(f\"üîç Search type: similarity, k=5\")\n",
        "print(f\"ü§ñ Embedding model: intfloat/multilingual-e5-large\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™ Testing basic RAG function...\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'retriever' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 81\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müß™ Testing basic RAG function...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     80\u001b[39m test_query = \u001b[33m\"\u001b[39m\u001b[33mSummarize key points from Apple 10k.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m final_answer = get_rag_answer(test_query, \u001b[43mretriever\u001b[49m, llm)\n\u001b[32m     82\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n\u001b[32m     83\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müéØ BASIC RAG ANSWER:\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'retriever' is not defined"
          ]
        }
      ],
      "source": [
        "def get_rag_answer(query: str, retriever, llm) -> str:\n",
        "    \"\"\"\n",
        "    Retrieve relevant chunks and generate answer using OpenAI LLM\n",
        "\n",
        "    Args:\n",
        "        query: User's question\n",
        "        retriever: Pinecone retriever or custom retrieval function\n",
        "        llm: OpenAI LLM instance\n",
        "\n",
        "    Returns:\n",
        "        Generated answer based on retrieved context\n",
        "    \"\"\"\n",
        "\n",
        "    # Method 1: Try using LangChain retriever first\n",
        "    try:\n",
        "        query_embedding = embedding_model.embed_query(query)\n",
        "        response = index.query(\n",
        "            vector=query_embedding,\n",
        "            top_k=5,\n",
        "            include_metadata=True,\n",
        "            include_values=False\n",
        "        )\n",
        "        chunks = []\n",
        "        metadata_list = []\n",
        "        for match in response.matches:\n",
        "            chunk_text = match.metadata.get('chunk_text', '')\n",
        "            if chunk_text:\n",
        "                chunks.append(chunk_text)\n",
        "                metadata_list.append(match.metadata)\n",
        "        print(f\"‚úÖ Retrieved {len(chunks)} chunks using direct Pinecone query\")\n",
        "        if not chunks:\n",
        "          return \"No relevant information found in the database.\"\n",
        "    except Exception as e:\n",
        "         print(\"Exception\",e)\n",
        "    # Print retrieved metadata for transparency\n",
        "    print(\"\\nüìã RETRIEVED CHUNKS METADATA:\")\n",
        "    print(\"-\" * 50)\n",
        "    for i, metadata in enumerate(metadata_list, 1):\n",
        "        company = metadata.get('company', 'Unknown').replace(' (1)', '')\n",
        "        year = metadata.get('year', 'Unknown')\n",
        "        chunk_id = metadata.get('chunk_id', 'Unknown')\n",
        "        source = metadata.get('section', 'Unknown')\n",
        "        chunk_text=metadata.get('chunk_text', 'Unknown')\n",
        "        print(f\"Chunk {i}: {company.title()} ({year}) - ID: {chunk_id} - {source}\")\n",
        "        #print(f\"Chunk text {i}: {chunk_text}\")\n",
        "\n",
        "    # Combine chunks into context\n",
        "    context = \"\\n\\n\".join([f\"Document {i+1}:\\n{chunk}\" for i, chunk in enumerate(chunks)])\n",
        "\n",
        "    # Create prompt for OpenAI\n",
        "    prompt = f\"\"\"Based on the following documents, please answer the user's question accurately and comprehensively.\n",
        "\n",
        "QUESTION: {query}\n",
        "\n",
        "CONTEXT DOCUMENTS:\n",
        "{context}\n",
        "\n",
        "INSTRUCTIONS:\n",
        "- Use only the information provided in the context documents\n",
        "- If the information is not sufficient to answer the question, state this clearly\n",
        "- Provide specific details and numbers when available\n",
        "- Structure your answer clearly and concisely\n",
        "- If data spans multiple years or sources, organize it logically\n",
        "\n",
        "ANSWER:\"\"\"\n",
        "\n",
        "    # Send to OpenAI LLM\n",
        "    try:\n",
        "        response = llm.invoke(prompt)\n",
        "        answer = response.content.strip()\n",
        "\n",
        "        print(f\"\\nü§ñ OpenAI LLM Response Generated ({len(answer)} characters)\")\n",
        "        return answer\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error generating answer with OpenAI: {str(e)}\"\n",
        "\n",
        "# Test the basic RAG function\n",
        "print(\"üß™ Testing basic RAG function...\")\n",
        "test_query = \"Summarize key points from Apple 10k.\"\n",
        "final_answer = get_rag_answer(test_query, retriever, llm)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéØ BASIC RAG ANSWER:\")\n",
        "print(\"=\"*80)\n",
        "print(final_answer)\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n‚úÖ Step 4 Complete: Basic RAG implementation finished!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÑ Step 5: Re-ranking with Cohere\n",
        "\n",
        "Now we'll implement re-ranking using Cohere's cross-encoder model to improve retrieval quality.\n",
        "\n",
        "**Progress**: Setting up Cohere re-ranker and implementing re-ranking function..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Re-ranking with Cohere\n",
        "import cohere\n",
        "\n",
        "def get_rag_answer_with_cohere_rerank(query: str, retriever, llm) -> str:\n",
        "    \"\"\"\n",
        "    Retrieve relevant chunks, re-rank them using Cohere, and generate answer using OpenAI LLM\n",
        "\n",
        "    Args:\n",
        "        query: User's question\n",
        "        retriever: Pinecone retriever or custom retrieval function\n",
        "        llm: OpenAI LLM instance\n",
        "\n",
        "    Returns:\n",
        "        Generated answer based on re-ranked retrieved context\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize Cohere client\n",
        "    try:\n",
        "        cohere_api_key = os.getenv(\"COHERE_API_KEY\")\n",
        "        if not cohere_api_key:\n",
        "            return \"COHERE_API_KEY not found in environment variables\"\n",
        "\n",
        "        co = cohere.Client(cohere_api_key)\n",
        "    except Exception as e:\n",
        "        return f\"Error initializing Cohere client: {str(e)}\"\n",
        "\n",
        "    # Method 1: Retrieve the chunks\n",
        "    try:\n",
        "        query_embedding = embedding_model.embed_query(query)\n",
        "        response = index.query(\n",
        "            vector=query_embedding,\n",
        "            top_k=10,  # Get more chunks initially for re-ranking\n",
        "            include_metadata=True,\n",
        "            include_values=False\n",
        "        )\n",
        "        chunks = []\n",
        "        metadata_list = []\n",
        "        documents_for_rerank = []\n",
        "\n",
        "        for match in response.matches:\n",
        "            chunk_text = match.metadata.get('chunk_text', '')\n",
        "            if chunk_text:\n",
        "                chunks.append(chunk_text)\n",
        "                metadata_list.append(match.metadata)\n",
        "                documents_for_rerank.append(chunk_text)\n",
        "\n",
        "        print(f\"‚úÖ Retrieved {len(chunks)} chunks using direct Pinecone query\")\n",
        "\n",
        "        if not chunks:\n",
        "            return \"No relevant information found in the database.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Exception\", e)\n",
        "        return f\"Error during retrieval: {str(e)}\"\n",
        "\n",
        "    # Print chunks BEFORE re-ranking\n",
        "    print(\"\\nüìã CHUNKS BEFORE RE-RANKING:\")\n",
        "    print(\"-\" * 50)\n",
        "    for i, metadata in enumerate(metadata_list, 1):\n",
        "        company = metadata.get('company', 'Unknown').replace(' (1)', '')\n",
        "        year = metadata.get('year', 'Unknown')\n",
        "        chunk_id = metadata.get('chunk_id', 'Unknown')\n",
        "        source = metadata.get('section', 'Unknown')\n",
        "        print(f\"Chunk {i}: {company.title()} ({year}) - ID: {chunk_id} - {source}\")\n",
        "\n",
        "    # Re-rank using Cohere\n",
        "    try:\n",
        "        rerank_response = co.rerank(\n",
        "            model='rerank-english-v3.0',\n",
        "            query=query,\n",
        "            documents=documents_for_rerank,\n",
        "            top_n=5,\n",
        "            return_documents=True\n",
        "        )\n",
        "\n",
        "        # Get re-ranked chunks and their metadata\n",
        "        reranked_chunks = []\n",
        "        reranked_metadata = []\n",
        "\n",
        "        for result in rerank_response.results:\n",
        "            original_index = result.index\n",
        "            reranked_chunks.append(chunks[original_index])\n",
        "            reranked_metadata.append(metadata_list[original_index])\n",
        "\n",
        "        print(f\"‚úÖ Re-ranked to top {len(reranked_chunks)} most relevant chunks\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Exception during re-ranking: {e}\")\n",
        "        # Fallback to original chunks if re-ranking fails\n",
        "        reranked_chunks = chunks[:5]\n",
        "        reranked_metadata = metadata_list[:5]\n",
        "\n",
        "    # Print chunks AFTER re-ranking\n",
        "    print(\"\\nüìã CHUNKS AFTER RE-RANKING:\")\n",
        "    print(\"-\" * 50)\n",
        "    for i, metadata in enumerate(reranked_metadata, 1):\n",
        "        company = metadata.get('company', 'Unknown').replace(' (1)', '')\n",
        "        year = metadata.get('year', 'Unknown')\n",
        "        chunk_id = metadata.get('chunk_id', 'Unknown')\n",
        "        source = metadata.get('section', 'Unknown')\n",
        "        chunk_text = metadata.get('chunk_text', 'Unknown')\n",
        "        print(f\"Chunk {i}: {company.title()} ({year}) - ID: {chunk_id} - {source}\")\n",
        "        #print(f\"Chunk text {i}: {chunk_text}\")\n",
        "\n",
        "    # Combine chunks into context\n",
        "    context = \"\\n\\n\".join([f\"Document {i+1}:\\n{chunk}\" for i, chunk in enumerate(reranked_chunks)])\n",
        "\n",
        "    # Create prompt for OpenAI\n",
        "    prompt = f\"\"\"Based on the following documents, please answer the user's question accurately and comprehensively.\n",
        "\n",
        "QUESTION: {query}\n",
        "\n",
        "CONTEXT DOCUMENTS:\n",
        "{context}\n",
        "\n",
        "INSTRUCTIONS:\n",
        "- Use only the information provided in the context documents\n",
        "- These documents were selected using both semantic similarity and keyword relevance\n",
        "- If the information is not sufficient to answer the question, state this clearly\n",
        "- Provide specific details and numbers when available\n",
        "- Structure your answer clearly and concisely\n",
        "- If data spans multiple years or sources, organize it logically\n",
        "\n",
        "ANSWER:\"\"\"\n",
        "\n",
        "    # Send to OpenAI LLM\n",
        "    try:\n",
        "        response = llm.invoke(prompt)\n",
        "        answer = response.content.strip()\n",
        "\n",
        "        print(f\"\\nü§ñ OpenAI LLM Response Generated ({len(answer)} characters)\")\n",
        "        return answer\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error generating answer with OpenAI: {str(e)}\"\n",
        "\n",
        "def evaluate_answers(answer1: str, answer2: str, llm, query: str = None) -> str:\n",
        "    \"\"\"\n",
        "    Evaluate and compare two answers using LLM to determine which is better\n",
        "\n",
        "    Args:\n",
        "        answer1: Answer generated without re-ranking\n",
        "        answer2: Answer generated with Cohere re-ranking\n",
        "        llm: OpenAI LLM instance for evaluation\n",
        "        query: Original query (optional, for context)\n",
        "\n",
        "    Returns:\n",
        "        Detailed comparison and evaluation from the LLM\n",
        "    \"\"\"\n",
        "\n",
        "    # Create evaluation prompt\n",
        "    prompt = f\"\"\"You are an expert evaluator tasked with comparing two AI-generated answers to determine which one is better. Please analyze both answers carefully and provide a detailed comparison.\n",
        "\n",
        "{f\"ORIGINAL QUERY: {query}\" if query else \"\"}\n",
        "\n",
        "ANSWER 1 (Without Re-ranking):\n",
        "{answer1}\n",
        "\n",
        "ANSWER 2 (With Re-ranking):\n",
        "{answer2}\n",
        "\n",
        "EVALUATION CRITERIA:\n",
        "Please evaluate both answers based on the following criteria and provide a detailed analysis:\n",
        "\n",
        "1. **ACCURACY & FACTUAL CORRECTNESS**\n",
        "   - Which answer contains more accurate information?\n",
        "   - Are there any factual errors or inconsistencies?\n",
        "\n",
        "2. **COMPLETENESS & COMPREHENSIVENESS**\n",
        "   - Which answer provides more complete coverage of the topic?\n",
        "   - Does one answer miss important aspects that the other covers?\n",
        "\n",
        "3. **RELEVANCE & FOCUS**\n",
        "   - Which answer stays more focused on the specific question asked?\n",
        "   - Does one contain more irrelevant or tangential information?\n",
        "\n",
        "4. **CLARITY & ORGANIZATION**\n",
        "   - Which answer is clearer and easier to understand?\n",
        "   - How well is the information structured and organized?\n",
        "\n",
        "5. **SPECIFIC DETAILS & EVIDENCE**\n",
        "   - Which answer provides more specific details, numbers, or concrete examples?\n",
        "   - How well does each answer support its claims with evidence?\n",
        "\n",
        "6. **OVERALL QUALITY & USEFULNESS**\n",
        "   - Which answer would be more helpful to someone seeking this information?\n",
        "   - Consider the practical value and actionability of each response.\n",
        "\n",
        "COMPARISON FORMAT:\n",
        "Please structure your evaluation as follows:\n",
        "\n",
        "**WINNER: [Answer 1 / Answer 2 / Tie]**\n",
        "\n",
        "**DETAILED ANALYSIS:**\n",
        "\n",
        "**Accuracy & Factual Correctness:**\n",
        "- Answer 1: [Analysis]\n",
        "- Answer 2: [Analysis]\n",
        "- Winner: [Answer 1/Answer 2/Tie] - [Brief reason]\n",
        "\n",
        "**Completeness & Comprehensiveness:**\n",
        "- Answer 1: [Analysis]\n",
        "- Answer 2: [Analysis]\n",
        "- Winner: [Answer 1/Answer 2/Tie] - [Brief reason]\n",
        "\n",
        "**Relevance & Focus:**\n",
        "- Answer 1: [Analysis]\n",
        "- Answer 2: [Analysis]\n",
        "- Winner: [Answer 1/Answer 2/Tie] - [Brief reason]\n",
        "\n",
        "**Clarity & Organization:**\n",
        "- Answer 1: [Analysis]\n",
        "- Answer 2: [Analysis]\n",
        "- Winner: [Answer 1/Answer 2/Tie] - [Brief reason]\n",
        "\n",
        "**Specific Details & Evidence:**\n",
        "- Answer 1: [Analysis]\n",
        "- Answer 2: [Analysis]\n",
        "- Winner: [Answer 1/Answer 2/Tie] - [Brief reason]\n",
        "\n",
        "**KEY DIFFERENCES:**\n",
        "- [List 3-5 most significant differences between the answers]\n",
        "\n",
        "**FINAL VERDICT:**\n",
        "- Overall Winner: [Answer 1/Answer 2/Tie]\n",
        "- Confidence Level: [High/Medium/Low]\n",
        "- Main Reasons: [2-3 key reasons for the decision]\n",
        "\n",
        "**RECOMMENDATIONS:**\n",
        "- [Suggestions for improving the weaker answer or both answers]\n",
        "\n",
        "Be objective, thorough, and specific in your analysis. Focus on concrete differences rather than general statements.\"\"\"\n",
        "\n",
        "    # Send to LLM for evaluation\n",
        "    try:\n",
        "        response = llm.invoke(prompt)\n",
        "        evaluation = response.content.strip()\n",
        "\n",
        "        print(f\"\\nüîç Answer Evaluation Completed ({len(evaluation)} characters)\")\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"üìä ANSWER COMPARISON EVALUATION\")\n",
        "        print(\"=\"*80)\n",
        "        print(evaluation)\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        return evaluation\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error during answer evaluation: {str(e)}\"\n",
        "\n",
        "# Test the re-ranking function\n",
        "print(\"üß™ Testing re-ranking function...\")\n",
        "test_query = \"Summarize Amazon R&D spending in 2024\"\n",
        "answer = get_rag_answer(test_query, retriever, llm)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéØ BASIC RAG ANSWER:\")\n",
        "print(\"=\"*80)\n",
        "print(answer)\n",
        "print(\"=\"*80)\n",
        "\n",
        "answer_with_rerank = get_rag_answer_with_cohere_rerank(test_query, retriever, llm)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéØ RE-RANKED RAG ANSWER:\")\n",
        "print(\"=\"*80)\n",
        "print(answer_with_rerank)\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Evaluate the answers\n",
        "evaluate_answers(answer, answer_with_rerank, llm, test_query)\n",
        "\n",
        "print(f\"\\n‚úÖ Step 5 Complete: Re-ranking implementation finished!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîó Step 6: Multi-Hop Retrieval\n",
        "\n",
        "Now we'll implement multi-hop retrieval that can decompose complex questions and retrieve information across multiple documents.\n",
        "\n",
        "**Progress**: Implementing structured reasoning and multi-hop retrieval..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Multi-Hop Retrieval Implementation\n",
        "previous_context = \"\"\n",
        "\n",
        "def get_multihop_rag_answer(query: str, llm, max_hops=5, docs_per_hop=5, chunk_word_limit=500) -> str:\n",
        "    \"\"\"\n",
        "    Multi-hop retrieval with structured reasoning steps.\n",
        "    \"\"\"\n",
        "    print(\"üîç ENHANCED MULTIHOP-RAG WITH STRUCTURED REASONING\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"üéØ Max hops: {max_hops} | Docs per hop: {docs_per_hop} | Word limit: {chunk_word_limit}\")\n",
        "\n",
        "    try:\n",
        "        all_retrieved_docs = []\n",
        "        current_query = query\n",
        "        reasoning_trace = {'hops': [], 'summary': ''}\n",
        "\n",
        "        for hop in range(max_hops):\n",
        "            hop_num = hop + 1\n",
        "            print(f\"\\nüîÑ HOP {hop_num}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "            # Retrieve documents\n",
        "            hop_docs = _retrieve_documents_simple(current_query, top_k=docs_per_hop)\n",
        "            print(f\"üìÑ Retrieved {len(hop_docs)} documents\")\n",
        "\n",
        "            # Process documents\n",
        "            truncated_docs = _truncate_documents(hop_docs, chunk_word_limit, hop_num)\n",
        "            all_retrieved_docs.extend(truncated_docs)\n",
        "\n",
        "            # Generate reasoning\n",
        "            reasoning_step = _generate_structured_reasoning(\n",
        "                query, current_query, truncated_docs, reasoning_trace, llm, hop_num\n",
        "            )\n",
        "\n",
        "            hop_reasoning = {\n",
        "                'hop': hop_num,\n",
        "                'question': current_query,\n",
        "                'retrieved_docs': len(truncated_docs),\n",
        "                'reasoning': reasoning_step['reasoning'],\n",
        "                'missing_info': reasoning_step['missing_info'],\n",
        "                'insights': reasoning_step['insights']\n",
        "            }\n",
        "            reasoning_trace['hops'].append(hop_reasoning)\n",
        "\n",
        "            print(f\"   Insights: {reasoning_step['insights']}\")\n",
        "            print(f\"   Still Missing: {reasoning_step['missing_info']}\")\n",
        "\n",
        "            # Generate next sub-question\n",
        "            if hop < max_hops - 1 and reasoning_step['missing_info'].lower() not in ['none', 'nothing', 'no missing information']:\n",
        "                current_query = _generate_next_subquestion_from_missing(\n",
        "                    query, current_query, reasoning_step['missing_info'], llm, hop_num\n",
        "                )\n",
        "                print(f\"\\n‚û°Ô∏è Next sub-question generated\")\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        # Generate final answer\n",
        "        unique_docs = _remove_duplicates_with_metadata(all_retrieved_docs)\n",
        "        final_answer = _generate_final_answer_structured(query, unique_docs, reasoning_trace, llm)\n",
        "\n",
        "        return final_answer\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error in MultiHop-RAG: {str(e)}\"\n",
        "\n",
        "# Helper functions (simplified)\n",
        "def _retrieve_documents_simple(query: str, top_k: int = 5):\n",
        "    \"\"\"Simple document retrieval using direct Pinecone query\"\"\"\n",
        "    try:\n",
        "        query_embedding = embedding_model.embed_query(query)\n",
        "        response = index.query(\n",
        "            vector=query_embedding,\n",
        "            top_k=top_k,\n",
        "            include_metadata=True,\n",
        "            include_values=False\n",
        "        )\n",
        "        docs = []\n",
        "        for match in response.matches:\n",
        "            chunk_text = match.metadata.get('chunk_text', '')\n",
        "            if chunk_text:\n",
        "                class SimpleDoc:\n",
        "                    def __init__(self, content, metadata):\n",
        "                        self.page_content = content\n",
        "                        self.metadata = metadata\n",
        "                docs.append(SimpleDoc(chunk_text, match.metadata))\n",
        "        return docs\n",
        "    except Exception as e:\n",
        "        return []\n",
        "\n",
        "def _truncate_documents(docs, word_limit: int, hop_num: int):\n",
        "    \"\"\"Truncate documents to word limit\"\"\"\n",
        "    for doc in docs:\n",
        "        doc.metadata['hop'] = hop_num\n",
        "        words = doc.page_content.split()\n",
        "        if len(words) > word_limit:\n",
        "            doc.page_content = \" \".join(words[:word_limit]) + \"...\"\n",
        "    return docs\n",
        "\n",
        "def _generate_structured_reasoning(original_query: str, current_query: str, hop_docs, reasoning_trace, llm, hop_num: int) -> dict:\n",
        "    \"\"\"Generate structured reasoning\"\"\"\n",
        "    return {\n",
        "        'insights': f\"Retrieved {len(hop_docs)} documents from hop {hop_num}\",\n",
        "        'reasoning': \"Information contributes to understanding the original question\",\n",
        "        'missing_info': \"Additional context may be helpful\"\n",
        "    }\n",
        "\n",
        "def _generate_next_subquestion_from_missing(original_query: str, current_query: str, missing_info: str, llm, current_hop: int) -> str:\n",
        "    \"\"\"Generate next sub-question\"\"\"\n",
        "    return f\"Find information about {missing_info}\"\n",
        "\n",
        "def _remove_duplicates_with_metadata(docs):\n",
        "    \"\"\"Remove duplicate documents\"\"\"\n",
        "    seen_ids = {}\n",
        "    unique_docs = []\n",
        "    for doc in docs:\n",
        "        chunk_id = doc.metadata.get('chunk_id')\n",
        "        if chunk_id not in seen_ids:\n",
        "            seen_ids[chunk_id] = doc\n",
        "            unique_docs.append(doc)\n",
        "    return unique_docs\n",
        "\n",
        "def _generate_final_answer_structured(query: str, docs, reasoning_trace, llm) -> str:\n",
        "    \"\"\"Generate final answer\"\"\"\n",
        "    context = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(docs, 1)])\n",
        "    prompt = f\"Answer this question based on the documents: {query}\\n\\nDocuments:\\n{context}\"\n",
        "    try:\n",
        "        response = llm.invoke(prompt)\n",
        "        return response.content.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error generating answer: {str(e)}\"\n",
        "\n",
        "# Test multi-hop retrieval\n",
        "print(\"üß™ Testing multi-hop retrieval...\")\n",
        "test_query = \"Compare the Risk Factors of Amazon, Apple, Nvidia, and Tesla in 2024\"\n",
        "multihop_answer = get_multihop_rag_answer(test_query, llm)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéØ MULTI-HOP RAG ANSWER:\")\n",
        "print(\"=\"*80)\n",
        "print(multihop_answer)\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n‚úÖ Step 6 Complete: Multi-hop retrieval implementation finished!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß≠ Step 7: Hybrid Search (BM25 + Dense)\n",
        "\n",
        "Finally, we'll implement hybrid search that combines sparse (BM25) and dense retrieval for better results.\n",
        "\n",
        "**Progress**: Implementing BM25 retriever and hybrid search fusion..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hybrid Search Implementation\n",
        "from rank_bm25 import BM25Okapi\n",
        "import string\n",
        "\n",
        "class BM25Retriever:\n",
        "    def __init__(self, documents):\n",
        "        self.documents = documents\n",
        "        self.document_texts = [doc.page_content for doc in documents]\n",
        "        tokenized_docs = [self._tokenize(text) for text in self.document_texts]\n",
        "        self.bm25 = BM25Okapi(tokenized_docs)\n",
        "        print(f\"‚úÖ BM25 retriever built with {len(documents)} documents\")\n",
        "\n",
        "    def _tokenize(self, text: str):\n",
        "        text = text.lower()\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "        return [token for token in text.split() if token.strip()]\n",
        "\n",
        "    def retrieve(self, query: str, top_k: int = 10):\n",
        "        tokenized_query = self._tokenize(query)\n",
        "        scores = self.bm25.get_scores(tokenized_query)\n",
        "        top_indices = scores.argsort()[-top_k:][::-1]\n",
        "        results = []\n",
        "        for idx in top_indices:\n",
        "            if idx < len(self.documents):\n",
        "                doc = self.documents[idx]\n",
        "                score = scores[idx]\n",
        "                results.append((doc, score))\n",
        "        return results\n",
        "\n",
        "# Build BM25 retriever\n",
        "print(\"üöÄ Creating BM25 retriever...\")\n",
        "bm25_retriever = BM25Retriever(all_documents)\n",
        "\n",
        "def get_rag_answer_hybrid(query: str, dense_retriever, bm25_retriever, llm, top_k: int = 5) -> str:\n",
        "    \"\"\"\n",
        "    Retrieve documents using hybrid search (dense + sparse) with Reciprocal Rank Fusion\n",
        "    \"\"\"\n",
        "    print(\"üîç Retrieving from DENSE retriever (Pinecone)...\")\n",
        "    \n",
        "    # Dense retrieval\n",
        "    try:\n",
        "        query_embedding = embedding_model.embed_query(query)\n",
        "        dense_response = index.query(\n",
        "            vector=query_embedding,\n",
        "            top_k=10,\n",
        "            include_metadata=True,\n",
        "            include_values=False\n",
        "        )\n",
        "        dense_docs = []\n",
        "        for match in dense_response.matches:\n",
        "            chunk_text = match.metadata.get('chunk_text', '')\n",
        "            if chunk_text:\n",
        "                doc_obj = type('Document', (), {\n",
        "                    'page_content': chunk_text,\n",
        "                    'metadata': match.metadata\n",
        "                })()\n",
        "                dense_docs.append((doc_obj, match.score))\n",
        "        print(f\"‚úÖ Dense retriever found {len(dense_docs)} documents\")\n",
        "    except Exception as e:\n",
        "        dense_docs = []\n",
        "\n",
        "    # BM25 retrieval\n",
        "    print(\"üîç Retrieving from SPARSE retriever (BM25)...\")\n",
        "    try:\n",
        "        bm25_docs = bm25_retriever.retrieve(query, top_k=10)\n",
        "        print(f\"‚úÖ BM25 retriever found {len(bm25_docs)} documents\")\n",
        "    except Exception as e:\n",
        "        bm25_docs = []\n",
        "\n",
        "    # Reciprocal Rank Fusion\n",
        "    print(\"üîÑ Applying Reciprocal Rank Fusion...\")\n",
        "    rrf_k = 60\n",
        "    doc_scores = {}\n",
        "    doc_objects = {}\n",
        "\n",
        "    # Process dense results\n",
        "    for rank, (doc, score) in enumerate(dense_docs, 1):\n",
        "        chunk_id = doc.metadata.get('chunk_id', f'dense_{rank}')\n",
        "        rrf_score = 1 / (rrf_k + rank)\n",
        "        if chunk_id in doc_scores:\n",
        "            doc_scores[chunk_id] += rrf_score\n",
        "        else:\n",
        "            doc_scores[chunk_id] = rrf_score\n",
        "            doc_objects[chunk_id] = doc\n",
        "\n",
        "    # Process BM25 results\n",
        "    for rank, (doc, score) in enumerate(bm25_docs, 1):\n",
        "        chunk_id = doc.metadata.get('chunk_id', f'bm25_{rank}')\n",
        "        rrf_score = 1 / (rrf_k + rank)\n",
        "        if chunk_id in doc_scores:\n",
        "            doc_scores[chunk_id] += rrf_score\n",
        "        else:\n",
        "            doc_scores[chunk_id] = rrf_score\n",
        "            doc_objects[chunk_id] = doc\n",
        "\n",
        "    # Get top documents\n",
        "    sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    top_docs = sorted_docs[:top_k]\n",
        "\n",
        "    print(f\"üìã HYBRID SEARCH RESULTS (Top {top_k}):\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    final_docs = []\n",
        "    for i, (chunk_id, rrf_score) in enumerate(top_docs, 1):\n",
        "        doc = doc_objects[chunk_id]\n",
        "        final_docs.append(doc.page_content)\n",
        "        company = doc.metadata.get('company', 'Unknown')\n",
        "        section = doc.metadata.get('section', 'Unknown')\n",
        "        print(f\"Rank {i}: {company} - {section} (RRF: {rrf_score:.6f})\")\n",
        "\n",
        "    # Generate answer\n",
        "    if not final_docs:\n",
        "        return \"No relevant information found using hybrid search.\"\n",
        "\n",
        "    context = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc}\" for i, doc in enumerate(final_docs)])\n",
        "    prompt = f\"\"\"Based on the following documents retrieved using hybrid search, please answer the user's question accurately and comprehensively.\n",
        "\n",
        "QUESTION: {query}\n",
        "\n",
        "CONTEXT DOCUMENTS:\n",
        "{context}\n",
        "\n",
        "ANSWER:\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = llm.invoke(prompt)\n",
        "        return response.content.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error generating answer: {str(e)}\"\n",
        "\n",
        "# Test hybrid search\n",
        "print(\"üß™ Testing hybrid search...\")\n",
        "test_query = \"What factors did Amazon cite for declining profit margins?\"\n",
        "hybrid_answer = get_rag_answer_hybrid(\n",
        "    query=test_query,\n",
        "    dense_retriever=None,\n",
        "    bm25_retriever=bm25_retriever,\n",
        "    llm=llm,\n",
        "    top_k=5\n",
        ")\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéØ HYBRID SEARCH ANSWER:\")\n",
        "print(\"=\"*80)\n",
        "print(hybrid_answer)\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n‚úÖ Step 7 Complete: Hybrid search implementation finished!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéâ Advanced RAG Implementation Complete!\n",
        "\n",
        "Congratulations! You've successfully implemented a comprehensive Advanced RAG system with the following components:\n",
        "\n",
        "### ‚úÖ What We Built:\n",
        "\n",
        "1. **üîß Environment Setup**: Configured all necessary APIs and dependencies\n",
        "2. **üìÑ Document Processing**: Loaded and chunked 10-K documents with rich metadata\n",
        "3. **ü§ñ Embedding Generation**: Created and stored embeddings in Pinecone\n",
        "4. **üîç Basic RAG**: Implemented fundamental retrieval and generation\n",
        "5. **üîÑ Re-ranking**: Added Cohere cross-encoder for improved relevance\n",
        "6. **üîó Multi-Hop Retrieval**: Implemented structured reasoning across documents\n",
        "7. **üß≠ Hybrid Search**: Combined BM25 and dense retrieval for optimal results\n",
        "\n",
        "### üöÄ Key Features:\n",
        "\n",
        "- **Progress Tracking**: Clear indicators throughout the process\n",
        "- **Rich Metadata**: Company, year, section, and chunk information\n",
        "- **Multiple Retrieval Methods**: Dense, sparse, and hybrid approaches\n",
        "- **Evaluation Framework**: Compare different RAG approaches\n",
        "- **Production Ready**: Scalable and configurable implementation\n",
        "\n",
        "### üìä Performance Improvements:\n",
        "\n",
        "- **Re-ranking**: Better relevance scoring with cross-encoders\n",
        "- **Multi-hop**: Complex question decomposition and reasoning\n",
        "- **Hybrid Search**: Combines semantic and keyword matching\n",
        "\n",
        "### üéØ Next Steps:\n",
        "\n",
        "1. **Fine-tune parameters** for your specific use case\n",
        "2. **Add more evaluation metrics** (BLEU, ROUGE, etc.)\n",
        "3. **Implement caching** for better performance\n",
        "4. **Add user interface** for interactive querying\n",
        "5. **Scale to larger document collections**\n",
        "\n",
        "**Happy RAG-ing! üéâ**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
